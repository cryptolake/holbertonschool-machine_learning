{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a85fa9ba-ce1f-492e-b2af-e3f736417f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 18:38:47.906623: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-13 18:38:47.906641: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12f234a7-2831-4d01-9541-18e5d7ac9c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"\n",
    "    Build a neural network with the Keras library\n",
    "\n",
    "    nx is the number of input features to the network\n",
    "\n",
    "    layers is a list containing the number of nodes\n",
    "    in each layer of the network\n",
    "\n",
    "    activations is a list containing the activation\n",
    "\n",
    "    functions used for each layer of the network\n",
    "\n",
    "    lambtha is the L2 regularization parameter\n",
    "\n",
    "    keep_prob is the probability that a node will\n",
    "    be kept for dropout\n",
    "\n",
    "    You are not allowed to use the Input class\n",
    "\n",
    "    Returns: the keras model\n",
    "    \"\"\"\n",
    "    model = K.Sequential()\n",
    "    for i, layer in enumerate(layers):\n",
    "        l2 = K.regularizers.L2(lambtha)\n",
    "        if i == 0:\n",
    "            model.add(K.layers.Dense(layer, input_shape=(nx,),\n",
    "                                     activation=activations[i],\n",
    "                                     kernel_regularizer=l2))\n",
    "        else:\n",
    "            model.add(K.layers.Dense(layer, \n",
    "                                     activation=activations[i],\n",
    "                                     kernel_regularizer=l2))\n",
    "        if i != len(layers)-1:\n",
    "            model.add(K.layers.Dropout(1-keep_prob))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd64eda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.38640815>, <tf.Tensor: shape=(), dtype=float32, numpy=0.25695384>, <tf.Tensor: shape=(), dtype=float32, numpy=0.019414766>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 18:38:58.414279: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-13 18:38:58.414316: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-13 18:38:58.414347: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (archpc): /proc/driver/nvidia/version does not exist\n",
      "2022-09-13 18:38:58.414648: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    network = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    network.summary()\n",
    "    print(network.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db066013",
   "metadata": {},
   "source": [
    "# You are not allowed to use the Sequential class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff137fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"\n",
    "    Build a neural network with the Keras library\n",
    "\n",
    "    nx is the number of input features to the network\n",
    "\n",
    "    layers is a list containing the number of nodes in\n",
    "    each layer of the network\n",
    "\n",
    "    activations is a list containing the activation functions\n",
    "    used for each layer of the network\n",
    "\n",
    "    lambtha is the L2 regularization parameter\n",
    "\n",
    "    keep_prob is the probability that a node will be kept for dropout\n",
    "    Returns: the keras model\n",
    "    \"\"\"\n",
    "    prev = K.Input(shape=(nx,))\n",
    "    inputs = prev\n",
    "    l2 = K.regularizers.L2(lambtha)\n",
    "    for i, layer in enumerate(layers):\n",
    "        prev = K.layers.Dense(layer, activation=activations[i], kernel_regularizer=l2)(prev)\n",
    "        if i != len(layers) - 1:\n",
    "            prev = K.layers.Dropout(keep_prob)(prev)\n",
    "\n",
    "    model = K.Model(inputs=inputs, outputs=prev)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69007df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 19:42:56.579719: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-13 19:42:56.579739: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.38640815>, <tf.Tensor: shape=(), dtype=float32, numpy=0.25695384>, <tf.Tensor: shape=(), dtype=float32, numpy=0.019414766>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 19:42:57.975802: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-13 19:42:57.975827: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-13 19:42:57.975852: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (archpc): /proc/driver/nvidia/version does not exist\n",
      "2022-09-13 19:42:57.976057: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    network = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    network.summary()\n",
    "    print(network.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "258a4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(network, alpha, beta1, beta2):\n",
    "    \"\"\"\n",
    "    sets up Adam optimization for a keras model with\\\n",
    "    categorical crossentropy loss and accuracy metrics.\n",
    "\n",
    "    network is the model to optimize\n",
    "    alpha is the learning rate\n",
    "    beta1 is the first Adam optimization parameter\n",
    "    beta2 is the second Adam optimization parameter\n",
    "    Returns: None \n",
    "    \"\"\"\n",
    "    optimizer = K.optimizers.Adam(alpha, beta1, beta2)\n",
    "    network.compile(optimizer=optimizer,\n",
    "                    loss=K.losses.CategoricalCrossentropy(),\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fd72b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.losses.CategoricalCrossentropy'>\n",
      "<class 'keras.optimizer_v2.adam.Adam'>\n",
      "(0.01, 0.99, 0.9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    optimize_model(model, 0.01, 0.99, 0.9)\n",
    "    print(model.loss)\n",
    "    opt = model.optimizer\n",
    "    print(opt.__class__)\n",
    "    print(tuple(map(lambda x: x.numpy(),(opt.lr, opt.beta_1, opt.beta_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "475ef9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, classes=None):\n",
    "    \"\"\"One hot encoding keras.\"\"\"\n",
    "    if classes is None:\n",
    "        classes = max(labels) + 1\n",
    "    layer = K.layers.CategoryEncoding(\n",
    "        num_tokens=classes, output_mode=\"one_hot\")\n",
    "    return layer(labels).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59be1a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    labels = np.load('../data/MNIST.npz')['Y_train'][:10]\n",
    "    print(labels)\n",
    "    print(one_hot(labels))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edafce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    Train Keras Model.\n",
    "\n",
    "    network is the model to train\n",
    "\n",
    "    data is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "\n",
    "    labels is a one-hot numpy.ndarray of shape (m, classes)\n",
    "     containing the labels of data\n",
    "\n",
    "    batch_size is the size of the batch used for mini-batch gradient descent\n",
    "\n",
    "    epochs is the number of passes through data for mini-batch gradient descent\n",
    "\n",
    "    verbose is a boolean that determines if output should be\n",
    "    printed during training\n",
    "\n",
    "    shuffle is a boolean that determines whether to\n",
    "    shuffle the batches every epoch.\n",
    "\n",
    "    Normally, it is a good idea to shuffle, but for reproducibility,\n",
    "    we have chosen to set the default to False.\n",
    "\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    history = network.fit(data, labels, batch_size, epochs,\n",
    "                          verbose=verbose, shuffle=shuffle)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f953ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 22:17:45.125496: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 156800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3256 - accuracy: 0.9210\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1748 - accuracy: 0.9658\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1409 - accuracy: 0.9754\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1249 - accuracy: 0.9803\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1147 - accuracy: 0.9839\n"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88f268bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                validation_data=None, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    Train Keras Model.\n",
    "\n",
    "    network is the model to train\n",
    "\n",
    "    data is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "\n",
    "    labels is a one-hot numpy.ndarray of shape (m, classes)\n",
    "     containing the labels of data\n",
    "\n",
    "    batch_size is the size of the batch used for mini-batch gradient descent\n",
    "\n",
    "    epochs is the number of passes through data for mini-batch gradient descent\n",
    "\n",
    "    verbose is a boolean that determines if output should be\n",
    "    printed during training\n",
    "\n",
    "    shuffle is a boolean that determines whether to\n",
    "    shuffle the batches every epoch.\n",
    "\n",
    "    Normally, it is a good idea to shuffle, but for reproducibility,\n",
    "    we have chosen to set the default to False.\n",
    "\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    history = network.fit(data, labels, batch_size, epochs,\n",
    "                          verbose=verbose, shuffle=shuffle,\n",
    "                          validation_data=validation_data)\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d85df1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 22:22:51.813670: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 156800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3256 - accuracy: 0.9210 - val_loss: 0.1889 - val_accuracy: 0.9628\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1748 - accuracy: 0.9658 - val_loss: 0.1586 - val_accuracy: 0.9700\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1409 - accuracy: 0.9754 - val_loss: 0.1555 - val_accuracy: 0.9724\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1249 - accuracy: 0.9803 - val_loss: 0.1529 - val_accuracy: 0.9738\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1147 - accuracy: 0.9839 - val_loss: 0.1510 - val_accuracy: 0.9748\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs, validation_data=(X_valid, Y_valid_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a13d810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network, data, labels, batch_size, epochs, \n",
    "                validation_data=None, early_stopping=False, \n",
    "                patience=0, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    Train Keras Model.\n",
    "\n",
    "    network is the model to train\n",
    "\n",
    "    data is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "\n",
    "    labels is a one-hot numpy.ndarray of shape (m, classes)\n",
    "     containing the labels of data\n",
    "\n",
    "    batch_size is the size of the batch used for mini-batch gradient descent\n",
    "\n",
    "    epochs is the number of passes through data for mini-batch gradient descent\n",
    "\n",
    "    verbose is a boolean that determines if output should be\n",
    "    printed during training\n",
    "\n",
    "    validation_data is the data to validate the model with, if not None\n",
    "\n",
    "    early_stopping is a boolean that indicates whether early stopping should be used\n",
    "\n",
    "    patience is the patience used for early stopping\n",
    "\n",
    "    shuffle is a boolean that determines whether to\n",
    "    shuffle the batches every epoch.\n",
    "\n",
    "    Normally, it is a good idea to shuffle, but for reproducibility,\n",
    "    we have chosen to set the default to False.\n",
    "\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    if validation_data:\n",
    "        early_stopping_callback = K.callbacks.EarlyStopping(\n",
    "            'val_loss', patience=patience)\n",
    "    history = network.fit(data, labels, batch_size, epochs,\n",
    "                          verbose=verbose, shuffle=shuffle,\n",
    "                          validation_data=validation_data,\n",
    "                          callbacks=early_stopping_callback)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40c5a660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3256 - accuracy: 0.9210 - val_loss: 0.1889 - val_accuracy: 0.9628\n",
      "Epoch 2/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1748 - accuracy: 0.9658 - val_loss: 0.1586 - val_accuracy: 0.9700\n",
      "Epoch 3/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1409 - accuracy: 0.9754 - val_loss: 0.1555 - val_accuracy: 0.9724\n",
      "Epoch 4/30\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1249 - accuracy: 0.9803 - val_loss: 0.1529 - val_accuracy: 0.9738\n",
      "Epoch 5/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1147 - accuracy: 0.9839 - val_loss: 0.1510 - val_accuracy: 0.9748\n",
      "Epoch 6/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1105 - accuracy: 0.9850 - val_loss: 0.1416 - val_accuracy: 0.9764\n",
      "Epoch 7/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1015 - accuracy: 0.9873 - val_loss: 0.1860 - val_accuracy: 0.9653\n",
      "Epoch 8/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1007 - accuracy: 0.9869 - val_loss: 0.1438 - val_accuracy: 0.9776\n",
      "Epoch 9/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0989 - accuracy: 0.9879 - val_loss: 0.1471 - val_accuracy: 0.9751\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 30\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "                validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "                patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd106298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                validation_data=None, early_stopping=False,\n",
    "                patience=0, learning_rate_decay=False,\n",
    "                alpha=0.1, decay_rate=1, verbose=True,\n",
    "                shuffle=False):\n",
    "    \"\"\"\n",
    "    Train Keras Model.\n",
    "\n",
    "    network is the model to train\n",
    "\n",
    "    data is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "\n",
    "    labels is a one-hot numpy.ndarray of shape (m, classes)\n",
    "     containing the labels of data\n",
    "\n",
    "    batch_size is the size of the batch used for mini-batch gradient descent\n",
    "\n",
    "    epochs is the number of passes through data for mini-batch gradient descent\n",
    "\n",
    "    verbose is a boolean that determines if output should be\n",
    "    printed during training\n",
    "\n",
    "    validation_data is the data to validate the model with, if not None\n",
    "\n",
    "    early_stopping is a boolean that indicates whether\n",
    "    early stopping should be used\n",
    "\n",
    "    patience is the patience used for early stopping\n",
    "\n",
    "    learning_rate_decay is a boolean that indicates whether learning rate\n",
    "    decay should be used\n",
    "\n",
    "    alpha is the initial learning rate\n",
    "\n",
    "    decay_rate is the decay rate\n",
    "\n",
    "    shuffle is a boolean that determines whether to\n",
    "    shuffle the batches every epoch.\n",
    "\n",
    "    Normally, it is a good idea to shuffle, but for reproducibility,\n",
    "    we have chosen to set the default to False.\n",
    "\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "\n",
    "    def schedule(epoch):\n",
    "        previous_lr = 1\n",
    "\n",
    "        def lr(epoch, start_lr, decay):\n",
    "            nonlocal previous_lr\n",
    "            previous_lr *= (start_lr / (1. + decay * epoch))\n",
    "            return previous_lr\n",
    "        return lr(epoch, alpha, decay_rate)\n",
    "\n",
    "    callbacks = []\n",
    "    if validation_data:\n",
    "        if early_stopping:\n",
    "            early_stopping_callback = K.callbacks.EarlyStopping(\n",
    "                'val_loss', patience=patience)\n",
    "            callbacks.append(early_stopping_callback)\n",
    "        if learning_rate_decay:\n",
    "            lr_callback = K.callbacks.LearningRateScheduler(\n",
    "                schedule, verbose=True)\n",
    "            callbacks.append(lr_callback)\n",
    "\n",
    "    history = network.fit(data, labels, batch_size, epochs,\n",
    "                          verbose=verbose, shuffle=shuffle,\n",
    "                          validation_data=validation_data,\n",
    "                          callbacks=callbacks)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62e099b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3256 - accuracy: 0.9210 - val_loss: 0.1889 - val_accuracy: 0.9628\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0005.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1596 - accuracy: 0.9698 - val_loss: 0.1473 - val_accuracy: 0.9727\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0003333333333333333.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1267 - accuracy: 0.9798 - val_loss: 0.1345 - val_accuracy: 0.9753\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.00025.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1097 - accuracy: 0.9847 - val_loss: 0.1275 - val_accuracy: 0.9765\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.0002.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0994 - accuracy: 0.9874 - val_loss: 0.1261 - val_accuracy: 0.9771\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.00016666666666666666.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0921 - accuracy: 0.9897 - val_loss: 0.1223 - val_accuracy: 0.9780\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.00014285714285714287.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0854 - accuracy: 0.9912 - val_loss: 0.1210 - val_accuracy: 0.9790\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.000125.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0809 - accuracy: 0.9924 - val_loss: 0.1203 - val_accuracy: 0.9795\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.00011111111111111112.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0783 - accuracy: 0.9933 - val_loss: 0.1175 - val_accuracy: 0.9798\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.0001.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0742 - accuracy: 0.9943 - val_loss: 0.1169 - val_accuracy: 0.9796\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 9.090909090909092e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0725 - accuracy: 0.9946 - val_loss: 0.1138 - val_accuracy: 0.9805\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 8.333333333333333e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0701 - accuracy: 0.9953 - val_loss: 0.1130 - val_accuracy: 0.9804\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 7.692307692307693e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0690 - accuracy: 0.9954 - val_loss: 0.1111 - val_accuracy: 0.9809\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 7.142857142857143e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0674 - accuracy: 0.9957 - val_loss: 0.1107 - val_accuracy: 0.9817\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 6.666666666666667e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0658 - accuracy: 0.9963 - val_loss: 0.1111 - val_accuracy: 0.9816\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 6.25e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0636 - accuracy: 0.9967 - val_loss: 0.1088 - val_accuracy: 0.9816\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 5.882352941176471e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0627 - accuracy: 0.9967 - val_loss: 0.1089 - val_accuracy: 0.9815\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 5.555555555555556e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0620 - accuracy: 0.9970 - val_loss: 0.1078 - val_accuracy: 0.9821\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 5.2631578947368424e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0613 - accuracy: 0.9970 - val_loss: 0.1087 - val_accuracy: 0.9821\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 5e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0601 - accuracy: 0.9974 - val_loss: 0.1077 - val_accuracy: 0.9822\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 4.761904761904762e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0591 - accuracy: 0.9974 - val_loss: 0.1063 - val_accuracy: 0.9824\n",
      "Epoch 22/1000\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 4.545454545454546e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0587 - accuracy: 0.9977 - val_loss: 0.1055 - val_accuracy: 0.9834\n",
      "Epoch 23/1000\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 4.347826086956522e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0579 - accuracy: 0.9976 - val_loss: 0.1060 - val_accuracy: 0.9827\n",
      "Epoch 24/1000\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 4.1666666666666665e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0573 - accuracy: 0.9979 - val_loss: 0.1060 - val_accuracy: 0.9828\n",
      "Epoch 25/1000\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 4e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0566 - accuracy: 0.9979 - val_loss: 0.1057 - val_accuracy: 0.9818\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 1000\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "                validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "                patience=3, learning_rate_decay=True, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f3f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                validation_data=None, early_stopping=False,\n",
    "                patience=0, learning_rate_decay=False,\n",
    "                alpha=0.1, decay_rate=1, save_best=False,\n",
    "                filepath=None, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    Train Keras Model.\n",
    "\n",
    "    network is the model to train\n",
    "\n",
    "    data is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "\n",
    "    labels is a one-hot numpy.ndarray of shape (m, classes)\n",
    "     containing the labels of data\n",
    "\n",
    "    batch_size is the size of the batch used for mini-batch gradient descent\n",
    "\n",
    "    epochs is the number of passes through data for mini-batch gradient descent\n",
    "\n",
    "    verbose is a boolean that determines if output should be\n",
    "    printed during training\n",
    "\n",
    "    validation_data is the data to validate the model with, if not None\n",
    "\n",
    "    early_stopping is a boolean that indicates whether\n",
    "    early stopping should be used\n",
    "\n",
    "    patience is the patience used for early stopping\n",
    "\n",
    "    learning_rate_decay is a boolean that indicates whether learning rate\n",
    "    decay should be used\n",
    "\n",
    "    alpha is the initial learning rate\n",
    "\n",
    "    decay_rate is the decay rate\n",
    "\n",
    "    save_best is a boolean indicating whether to save the model\n",
    "    after each epoch if it is the best\n",
    "\n",
    "    filepath is the file path where the model should be saved\n",
    "\n",
    "    shuffle is a boolean that determines whether to\n",
    "    shuffle the batches every epoch.\n",
    "\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "\n",
    "    def schedule(epoch):\n",
    "        previous_lr = 1\n",
    "\n",
    "        def lr(epoch, start_lr, decay):\n",
    "            nonlocal previous_lr\n",
    "            previous_lr *= (start_lr / (1. + decay * epoch))\n",
    "            return previous_lr\n",
    "        return lr(epoch, alpha, decay_rate)\n",
    "\n",
    "    callbacks = []\n",
    "    if validation_data:\n",
    "        if early_stopping:\n",
    "            early_stopping_callback = K.callbacks.EarlyStopping(\n",
    "                'val_loss', patience=patience)\n",
    "            callbacks.append(early_stopping_callback)\n",
    "        if learning_rate_decay:\n",
    "            lr_callback = K.callbacks.LearningRateScheduler(\n",
    "                schedule, verbose=True)\n",
    "            callbacks.append(lr_callback)\n",
    "        checkpoint = K.callbacks.ModelCheckpoint(filepath=filepath,\n",
    "                                                 save_best_only=save_best)\n",
    "        callbacks.append(checkpoint)\n",
    "\n",
    "    history = network.fit(data, labels, batch_size, epochs,\n",
    "                          verbose=verbose, shuffle=shuffle,\n",
    "                          validation_data=validation_data,\n",
    "                          callbacks=callbacks)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "383583ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                validation_data=None, early_stopping=False,\n",
    "                patience=0, learning_rate_decay=False,\n",
    "                alpha=0.1, decay_rate=1, save_best=False,\n",
    "                filepath=None, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    Train Keras Model.\n",
    "\n",
    "    network is the model to train\n",
    "\n",
    "    data is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "\n",
    "    labels is a one-hot numpy.ndarray of shape (m, classes)\n",
    "     containing the labels of data\n",
    "\n",
    "    batch_size is the size of the batch used for mini-batch gradient descent\n",
    "\n",
    "    epochs is the number of passes through data for mini-batch gradient descent\n",
    "\n",
    "    verbose is a boolean that determines if output should be\n",
    "    printed during training\n",
    "\n",
    "    validation_data is the data to validate the model with, if not None\n",
    "\n",
    "    early_stopping is a boolean that indicates whether\n",
    "    early stopping should be used\n",
    "\n",
    "    patience is the patience used for early stopping\n",
    "\n",
    "    learning_rate_decay is a boolean that indicates whether learning rate\n",
    "    decay should be used\n",
    "\n",
    "    alpha is the initial learning rate\n",
    "\n",
    "    decay_rate is the decay rate\n",
    "\n",
    "    save_best is a boolean indicating whether to save the model\n",
    "    after each epoch if it is the best\n",
    "\n",
    "    filepath is the file path where the model should be saved\n",
    "\n",
    "    shuffle is a boolean that determines whether to\n",
    "    shuffle the batches every epoch.\n",
    "\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "\n",
    "    def schedule(epoch):\n",
    "        previous_lr = 1\n",
    "\n",
    "        def lr(epoch, start_lr, decay):\n",
    "            nonlocal previous_lr\n",
    "            previous_lr *= (start_lr / (1. + decay * epoch))\n",
    "            return previous_lr\n",
    "        return lr(epoch, alpha, decay_rate)\n",
    "\n",
    "    callbacks = []\n",
    "    if validation_data:\n",
    "        if early_stopping:\n",
    "            early_stopping_callback = K.callbacks.EarlyStopping(\n",
    "                'val_loss', patience=patience)\n",
    "            callbacks.append(early_stopping_callback)\n",
    "        if learning_rate_decay:\n",
    "            lr_callback = K.callbacks.LearningRateScheduler(\n",
    "                schedule, verbose=True)\n",
    "            callbacks.append(lr_callback)\n",
    "        if filepath:\n",
    "            checkpoint = K.callbacks.ModelCheckpoint(filepath=filepath,\n",
    "                                                    save_best_only=save_best)\n",
    "            callbacks.append(checkpoint)\n",
    "\n",
    "    history = network.fit(data, labels, batch_size, epochs,\n",
    "                          verbose=verbose, shuffle=shuffle,\n",
    "                          validation_data=validation_data,\n",
    "                          callbacks=callbacks)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2078bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3256 - accuracy: 0.9210 - val_loss: 0.1889 - val_accuracy: 0.9628\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0005.\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1596 - accuracy: 0.9698 - val_loss: 0.1473 - val_accuracy: 0.9727\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0003333333333333333.\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1267 - accuracy: 0.9798 - val_loss: 0.1345 - val_accuracy: 0.9753\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.00025.\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1097 - accuracy: 0.9847 - val_loss: 0.1275 - val_accuracy: 0.9765\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.0002.\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.0994 - accuracy: 0.9874 - val_loss: 0.1261 - val_accuracy: 0.9771\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.00016666666666666666.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0921 - accuracy: 0.9897 - val_loss: 0.1223 - val_accuracy: 0.9780\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.00014285714285714287.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0854 - accuracy: 0.9912 - val_loss: 0.1210 - val_accuracy: 0.9790\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.000125.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0809 - accuracy: 0.9924 - val_loss: 0.1203 - val_accuracy: 0.9795\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.00011111111111111112.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0783 - accuracy: 0.9933 - val_loss: 0.1175 - val_accuracy: 0.9798\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.0001.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0742 - accuracy: 0.9943 - val_loss: 0.1169 - val_accuracy: 0.9796\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 9.090909090909092e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0725 - accuracy: 0.9946 - val_loss: 0.1138 - val_accuracy: 0.9805\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 8.333333333333333e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0701 - accuracy: 0.9953 - val_loss: 0.1130 - val_accuracy: 0.9804\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 7.692307692307693e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0690 - accuracy: 0.9954 - val_loss: 0.1111 - val_accuracy: 0.9809\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 7.142857142857143e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0674 - accuracy: 0.9957 - val_loss: 0.1107 - val_accuracy: 0.9817\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 6.666666666666667e-05.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0658 - accuracy: 0.9963 - val_loss: 0.1111 - val_accuracy: 0.9816\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 6.25e-05.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0636 - accuracy: 0.9967 - val_loss: 0.1088 - val_accuracy: 0.9816\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 5.882352941176471e-05.\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0627 - accuracy: 0.9967 - val_loss: 0.1089 - val_accuracy: 0.9815\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 5.555555555555556e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0620 - accuracy: 0.9970 - val_loss: 0.1078 - val_accuracy: 0.9821\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 5.2631578947368424e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0613 - accuracy: 0.9970 - val_loss: 0.1087 - val_accuracy: 0.9821\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 5e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0601 - accuracy: 0.9974 - val_loss: 0.1077 - val_accuracy: 0.9822\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 4.761904761904762e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0591 - accuracy: 0.9974 - val_loss: 0.1063 - val_accuracy: 0.9824\n",
      "Epoch 22/1000\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 4.545454545454546e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0587 - accuracy: 0.9977 - val_loss: 0.1055 - val_accuracy: 0.9834\n",
      "Epoch 23/1000\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 4.347826086956522e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0579 - accuracy: 0.9976 - val_loss: 0.1060 - val_accuracy: 0.9827\n",
      "Epoch 24/1000\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 4.1666666666666665e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0573 - accuracy: 0.9979 - val_loss: 0.1060 - val_accuracy: 0.9828\n",
      "Epoch 25/1000\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 4e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0566 - accuracy: 0.9979 - val_loss: 0.1057 - val_accuracy: 0.9818\n"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 1000\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "                validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "                patience=3, learning_rate_decay=True, alpha=alpha,\n",
    "                save_best=True, filepath='network1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b1d339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model(network, filename):\n",
    "    \"\"\"Save model.\"\"\"\n",
    "    K.models.save_model(network, filename)\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"Load model.\"\"\"\n",
    "    model = K.models.load_model(filename)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd106072",
   "metadata": {},
   "source": [
    "This is how to load model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4edfa1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.1775 - accuracy: 0.9636 - val_loss: 0.1673 - val_accuracy: 0.9700\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0005.\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1064 - accuracy: 0.9859 - val_loss: 0.1348 - val_accuracy: 0.9787\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0003333333333333333.\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.0843 - accuracy: 0.9921 - val_loss: 0.1261 - val_accuracy: 0.9800\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.00025.\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.0740 - accuracy: 0.9941 - val_loss: 0.1209 - val_accuracy: 0.9800\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.0002.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0669 - accuracy: 0.9958 - val_loss: 0.1154 - val_accuracy: 0.9816\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.00016666666666666666.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0631 - accuracy: 0.9965 - val_loss: 0.1096 - val_accuracy: 0.9826\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.00014285714285714287.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0585 - accuracy: 0.9974 - val_loss: 0.1111 - val_accuracy: 0.9822\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.000125.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0573 - accuracy: 0.9973 - val_loss: 0.1059 - val_accuracy: 0.9833\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.00011111111111111112.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0546 - accuracy: 0.9979 - val_loss: 0.1066 - val_accuracy: 0.9822\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0532 - accuracy: 0.9980 - val_loss: 0.1034 - val_accuracy: 0.9830\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 9.090909090909092e-05.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0518 - accuracy: 0.9980 - val_loss: 0.1017 - val_accuracy: 0.9830\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 8.333333333333333e-05.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0503 - accuracy: 0.9985 - val_loss: 0.1020 - val_accuracy: 0.9828\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 7.692307692307693e-05.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0493 - accuracy: 0.9986 - val_loss: 0.1011 - val_accuracy: 0.9827\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 7.142857142857143e-05.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0484 - accuracy: 0.9985 - val_loss: 0.0989 - val_accuracy: 0.9836\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 6.666666666666667e-05.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0475 - accuracy: 0.9987 - val_loss: 0.0980 - val_accuracy: 0.9831\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 6.25e-05.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0463 - accuracy: 0.9990 - val_loss: 0.0972 - val_accuracy: 0.9835\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 5.882352941176471e-05.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0462 - accuracy: 0.9990 - val_loss: 0.0961 - val_accuracy: 0.9839\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 5.555555555555556e-05.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0457 - accuracy: 0.9988 - val_loss: 0.0964 - val_accuracy: 0.9840\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 5.2631578947368424e-05.\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.0452 - accuracy: 0.9989 - val_loss: 0.0963 - val_accuracy: 0.9832\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[array([[-1.7844683e-32,  1.3623944e-32, -2.4625073e-32, ...,\n",
      "        -9.0704465e-33,  5.7878127e-32, -4.8722243e-32],\n",
      "       [ 2.9494564e-32,  1.8965859e-33,  6.8861848e-33, ...,\n",
      "        -3.9435720e-32,  3.1167328e-32,  5.0216619e-33],\n",
      "       [ 5.5530982e-32,  5.5159943e-32,  3.2902910e-33, ...,\n",
      "        -4.8895792e-32, -2.4721804e-32, -4.4585713e-32],\n",
      "       ...,\n",
      "       [-4.5627166e-33, -3.5246290e-32, -2.3581713e-32, ...,\n",
      "        -4.7059988e-32, -3.5589055e-32,  4.9196449e-32],\n",
      "       [-1.3099095e-32, -3.1499416e-32,  1.3375104e-32, ...,\n",
      "        -4.6898945e-32,  4.7419871e-32, -4.7809824e-33],\n",
      "       [-4.6035533e-33, -3.6502626e-33, -6.8788680e-34, ...,\n",
      "        -1.6749020e-32,  2.6938310e-32, -4.5302153e-32]], dtype=float32), array([ 4.41116244e-02, -2.20513046e-02, -2.29105316e-02, -3.01714372e-02,\n",
      "        9.57919098e-03, -8.95479396e-02,  2.29536798e-02,  4.95737605e-02,\n",
      "       -1.30351987e-02, -3.50225531e-02,  6.55911639e-02, -1.57437976e-02,\n",
      "       -3.59149054e-02, -5.23337722e-02,  5.91720007e-02, -8.53534974e-03,\n",
      "        2.16392875e-02,  5.08685037e-03, -4.02847342e-02, -4.97057987e-03,\n",
      "       -8.83265212e-02, -4.57055867e-03, -6.81671426e-02,  1.27231956e-01,\n",
      "        3.02412212e-02,  2.24127583e-02, -5.79890683e-02,  2.95017492e-02,\n",
      "       -1.30201327e-02, -1.28436806e-02,  1.25954933e-02,  2.89599355e-02,\n",
      "        7.56247900e-03, -1.28780037e-01, -1.32086396e-01,  8.72921646e-02,\n",
      "       -1.44768432e-02, -3.53562683e-02,  8.56259614e-02,  1.61070470e-02,\n",
      "       -1.44630019e-02, -2.28572562e-02, -2.22460292e-02, -4.44116294e-02,\n",
      "       -2.66413651e-02,  3.77606452e-02,  3.90675552e-02, -1.98412109e-02,\n",
      "       -8.50456133e-02,  2.24378258e-02,  4.26793331e-03, -2.41539571e-02,\n",
      "       -9.18617938e-03, -1.20685035e-02, -5.44183403e-02,  7.29259625e-02,\n",
      "       -4.62926216e-02, -2.43727285e-02, -9.95587185e-03, -1.89634156e-03,\n",
      "        3.46820764e-02,  7.39797354e-02, -9.74316448e-02, -1.51490634e-02,\n",
      "       -1.50165493e-02, -5.24311513e-02,  4.61248728e-03,  3.71900834e-02,\n",
      "       -4.99547347e-02, -1.67943295e-02,  7.38122761e-02,  3.51653025e-02,\n",
      "       -1.54415462e-02, -1.29496958e-02, -5.57271205e-02, -1.83183309e-02,\n",
      "        4.48527224e-02,  4.67452370e-02,  5.14115468e-02, -4.33305884e-03,\n",
      "       -9.31677874e-03,  2.59251781e-02, -5.71389906e-02,  1.62688587e-02,\n",
      "        1.48283439e-02,  2.01325188e-03,  9.84209962e-03,  1.48799300e-01,\n",
      "       -1.09781651e-02, -1.61458552e-02, -7.36314012e-03,  7.15116784e-03,\n",
      "       -1.08879374e-03, -4.52603139e-02,  6.50206059e-02,  2.49094293e-02,\n",
      "        1.87849645e-02,  8.93601626e-02,  3.93521003e-02, -4.14865613e-02,\n",
      "        5.98873347e-02, -3.40173654e-02,  1.69799905e-02, -4.65587713e-02,\n",
      "        5.43219261e-02, -1.84993222e-02,  1.05425298e-01,  6.60060858e-03,\n",
      "       -2.59678462e-04, -1.22658825e-02, -2.65405942e-02,  8.81480053e-02,\n",
      "       -3.17971371e-02, -2.69421400e-03,  4.02777866e-02,  5.94804883e-02,\n",
      "        2.96249315e-02, -1.98287517e-02, -9.65998322e-02, -4.00327370e-02,\n",
      "       -1.08237145e-02,  2.48760413e-02, -4.01539030e-03, -2.38805515e-04,\n",
      "        4.75521162e-02, -2.74467729e-02,  8.28831792e-02,  5.56050539e-02,\n",
      "        8.72438103e-02, -1.48192002e-02, -1.44244656e-02,  4.94377278e-02,\n",
      "        5.28288186e-02,  5.07488772e-02,  3.87453027e-02, -2.47374345e-02,\n",
      "       -7.37827495e-02,  4.34840582e-02,  3.70356776e-02,  2.60882713e-02,\n",
      "        5.02078496e-02, -4.86939866e-03, -4.01832312e-02,  6.59602806e-02,\n",
      "        4.10662666e-02,  9.17150639e-03, -2.42466237e-02, -2.67401468e-02,\n",
      "       -5.97764214e-04,  7.10305339e-03,  1.60129238e-02, -2.15408336e-02,\n",
      "        6.86729029e-02,  2.43976479e-03, -3.00747100e-02, -4.77475673e-03,\n",
      "       -9.61354189e-03, -4.52234857e-02,  1.39340937e-01,  2.93087680e-03,\n",
      "        2.21187267e-02,  1.70205496e-02,  7.35703632e-02, -1.92328133e-02,\n",
      "       -3.58944014e-02, -1.68813672e-02,  1.96955986e-02,  4.73264456e-02,\n",
      "       -2.21782718e-02, -4.95152967e-03,  1.16515741e-01, -2.54204404e-02,\n",
      "       -4.18819971e-02,  2.58312896e-02,  1.26664847e-01,  5.62267238e-03,\n",
      "        7.76086301e-02,  2.50084680e-02,  1.42398970e-02,  7.74948075e-02,\n",
      "        4.90757972e-02, -2.83874776e-02,  3.15661803e-02,  3.52093764e-02,\n",
      "       -4.29430753e-02, -2.80730147e-02, -8.90930817e-02, -1.54408338e-02,\n",
      "       -7.75257573e-02, -4.65556368e-05,  4.76387404e-02,  4.93010767e-02,\n",
      "       -6.08423464e-02, -6.14856370e-03, -9.09451842e-02,  8.85374993e-02,\n",
      "        2.56177550e-03,  5.66253625e-02,  1.87379401e-02, -2.83822976e-02,\n",
      "        3.19507867e-02,  4.07592952e-02, -6.73909578e-03,  4.18129563e-02,\n",
      "       -5.79973171e-03, -1.30386523e-03, -4.70712371e-02,  1.41945668e-02,\n",
      "       -4.89693275e-03,  9.99397710e-02, -2.34237090e-02,  4.56329137e-02,\n",
      "        6.47003576e-02, -1.96036212e-02,  2.48392746e-02,  3.56042981e-02,\n",
      "       -1.03055139e-03,  1.33999735e-01,  5.39684892e-02, -2.92906072e-02,\n",
      "       -6.61796853e-02, -7.32746795e-02,  6.27827048e-02,  2.56896764e-02,\n",
      "        5.48328944e-02,  1.60031449e-02, -1.11514153e-02,  3.93619761e-02,\n",
      "        3.35754640e-02, -1.62827596e-02, -4.80255596e-02,  3.87622118e-02,\n",
      "        6.92419708e-02, -6.95667043e-03, -2.74305418e-02, -6.58746734e-02,\n",
      "       -2.14973260e-02,  7.08335266e-03, -1.92379970e-02,  2.03654580e-02,\n",
      "        2.32839044e-02,  2.05007736e-02, -4.87987176e-02, -4.75638692e-04,\n",
      "        7.49004409e-02,  1.74893960e-02, -1.52621733e-03, -3.02197002e-02,\n",
      "       -1.03032909e-01, -8.25000554e-02,  1.05432354e-01,  1.67104546e-02,\n",
      "        6.88468888e-02,  2.40296610e-02,  9.87408310e-02,  5.03101759e-03],\n",
      "      dtype=float32), array([[-3.8074231e-31,  3.6164004e-02, -3.7465550e-03, ...,\n",
      "         2.9137393e-02, -4.1835845e-02, -1.9035531e-02],\n",
      "       [ 4.9093012e-31, -3.2019012e-02,  9.0279700e-03, ...,\n",
      "        -7.6891497e-02,  4.6194280e-03,  2.3395095e-02],\n",
      "       [-3.8965235e-31, -5.5067915e-02,  5.0139152e-02, ...,\n",
      "        -3.6282785e-02,  1.9902220e-02,  7.4532561e-02],\n",
      "       ...,\n",
      "       [-3.8922588e-31,  1.1348985e-02, -1.5724810e-02, ...,\n",
      "         2.0718284e-02,  3.4585107e-02, -4.4128291e-02],\n",
      "       [-4.5730459e-31,  9.7271644e-02, -4.9753856e-02, ...,\n",
      "         1.1498870e-01, -4.0233484e-03,  7.0252810e-03],\n",
      "       [-4.8305666e-31, -3.9849401e-02,  2.5747441e-02, ...,\n",
      "        -1.5135340e-02,  6.5027624e-02,  2.3806401e-02]], dtype=float32), array([-8.14932361e-02,  4.74461960e-03, -1.57375764e-02,  5.75402714e-02,\n",
      "        1.56591043e-01, -3.24418657e-02,  3.43626216e-02, -1.53806582e-02,\n",
      "       -2.37967484e-02,  9.46400911e-02, -8.82545561e-02, -6.36462774e-03,\n",
      "        7.97297060e-02,  3.94200459e-02, -1.12368660e-02,  6.02367055e-03,\n",
      "        1.56496897e-01,  1.96969613e-01, -8.86230264e-03,  1.43265158e-01,\n",
      "        7.34163856e-04,  8.10091868e-02,  8.41038376e-02,  5.63556142e-02,\n",
      "       -2.51622088e-02,  6.27735630e-02,  2.54746750e-02,  2.83681229e-02,\n",
      "       -5.60860299e-02, -4.54483442e-02,  1.42771423e-01,  1.67926461e-01,\n",
      "        1.10653289e-01,  1.84535682e-01,  9.58856121e-02,  1.31630361e-01,\n",
      "        6.37976453e-02,  4.56525087e-02,  1.24812303e-02,  1.30016282e-01,\n",
      "       -2.19346993e-02,  1.03227451e-01,  9.06669945e-02,  9.08903256e-02,\n",
      "       -4.83336626e-03,  3.35617624e-02,  9.25840214e-02,  1.58889242e-03,\n",
      "       -5.48756160e-02,  1.50993809e-01,  1.67745441e-01,  9.04000849e-02,\n",
      "        3.38690472e-03,  1.57068342e-01,  4.27833982e-02,  7.09905922e-02,\n",
      "       -2.75579896e-02,  1.06946968e-01, -4.37989309e-02,  3.09817772e-02,\n",
      "        1.53020009e-01, -5.40502090e-03,  9.26020443e-02, -3.24947871e-02,\n",
      "        6.79544881e-02,  3.49818292e-04,  7.18823029e-03,  1.74483687e-01,\n",
      "        4.02093818e-03,  2.82175802e-02,  2.59699933e-02,  1.11287400e-01,\n",
      "        1.59175038e-01, -4.78900820e-02, -2.22665519e-02, -3.13159497e-03,\n",
      "        7.17234192e-03,  7.58898333e-02, -5.16683757e-02,  1.51841026e-02,\n",
      "       -2.30279211e-02,  9.86637399e-02,  1.11019894e-01,  1.31028831e-01,\n",
      "        1.14065278e-02,  1.61418602e-01,  5.15416935e-02, -6.01639226e-02,\n",
      "        6.31287917e-02,  2.13980302e-02,  1.38996527e-01, -1.82353165e-02,\n",
      "        1.85423017e-01, -9.43167694e-03,  1.28725454e-01,  8.86330083e-02,\n",
      "       -1.63685698e-02,  1.24633417e-01,  9.94081125e-02,  1.52303293e-01,\n",
      "        2.02982366e-01,  9.09324661e-02,  1.24291018e-01,  8.41214023e-06,\n",
      "        7.66754821e-02,  5.04045747e-02,  5.64191630e-03, -5.75296879e-02,\n",
      "       -1.65306702e-02,  8.18219185e-02,  6.66683316e-02,  2.13128533e-02,\n",
      "       -4.91340607e-02, -7.65508115e-02,  2.27158889e-02,  4.59107906e-02,\n",
      "        1.14315987e-01,  5.40230833e-02,  4.42199782e-02,  1.56954989e-01,\n",
      "        8.97869691e-02,  2.94518918e-02,  4.71440405e-02, -3.19887772e-02,\n",
      "       -6.98238984e-02, -2.83639859e-02,  1.51739538e-01, -6.58504814e-02,\n",
      "       -1.45611642e-02, -1.65885054e-02,  1.00904875e-01, -8.82754847e-03,\n",
      "       -7.99267590e-02,  2.93533318e-02,  1.47569314e-01,  1.12509772e-01,\n",
      "        1.34589849e-02, -6.31046817e-02,  2.58851778e-02,  5.49162040e-03,\n",
      "        6.36460185e-02, -6.60989759e-03,  1.85025647e-01,  1.51599973e-01,\n",
      "       -3.63826044e-02,  5.05557060e-02,  4.15443517e-02,  2.87578232e-03,\n",
      "       -8.72953515e-03,  1.10034183e-01, -3.79142836e-02,  7.23893642e-02,\n",
      "       -6.64181188e-02,  1.09862380e-01,  3.85375582e-02,  6.75506368e-02,\n",
      "        1.58621877e-01, -5.64608723e-02,  1.82081416e-01, -4.99807559e-02,\n",
      "        2.34852526e-02, -4.01211940e-02,  9.96668562e-02,  9.48135406e-02,\n",
      "       -5.17400503e-02, -9.22965705e-02, -2.24165022e-02, -1.05068097e-02,\n",
      "        2.31061317e-03,  1.03694117e-02,  8.37301910e-02,  5.30177392e-02,\n",
      "        5.32704815e-02,  6.21215403e-02,  7.71790668e-02, -1.25054875e-02,\n",
      "        1.52298152e-01,  6.90092817e-02, -7.52527639e-02, -6.74139149e-03,\n",
      "        1.95869375e-02,  9.59277898e-03, -3.69702466e-02, -5.17258607e-02,\n",
      "        1.77846514e-02, -2.70704404e-02,  8.31003413e-02,  3.19901183e-02,\n",
      "        4.80074883e-02, -3.30440849e-02, -3.56207825e-02, -7.83505011e-03,\n",
      "        1.87132746e-01, -6.81807622e-02,  7.48498812e-02,  1.77136406e-01,\n",
      "        2.80063152e-02,  1.87921580e-02,  1.21084586e-01,  1.04545623e-01,\n",
      "        1.94921643e-02,  1.95906967e-01, -4.01193183e-03,  9.18128118e-02,\n",
      "        4.43472452e-02,  1.05904356e-01,  3.98264220e-03, -1.97361391e-02,\n",
      "       -2.51582563e-02,  6.25557527e-02,  1.30449384e-01, -7.74362758e-02,\n",
      "       -3.20490785e-02,  8.56067389e-02,  1.80949003e-01, -5.05657084e-02,\n",
      "       -1.81537662e-02,  1.31997004e-01,  5.56533644e-03,  1.65286735e-02,\n",
      "       -3.40857320e-02,  2.02900931e-01, -1.40265608e-02,  7.60712847e-02,\n",
      "        2.53461245e-02,  8.59234780e-02,  1.61580276e-02,  1.10857859e-02,\n",
      "       -4.44391323e-03,  3.66279259e-02,  8.38291273e-02,  7.65340179e-02,\n",
      "        9.00034606e-02,  5.18020317e-02,  7.79718626e-03,  7.96637684e-02,\n",
      "       -2.03274097e-02,  1.23810470e-01,  2.45951880e-02, -8.46773945e-03,\n",
      "        1.31936520e-01, -9.58304759e-03,  1.63948044e-01,  1.42533928e-01,\n",
      "        7.14992285e-02,  1.32686168e-01,  1.79248378e-02, -3.55118848e-02,\n",
      "       -5.70958294e-02,  1.12110689e-01,  1.38645172e-01,  1.29469680e-02,\n",
      "       -1.94553472e-02,  2.28653643e-02,  1.64320797e-01, -3.22366890e-04],\n",
      "      dtype=float32), array([[-3.94860448e-31,  5.23516848e-31, -4.81263233e-31, ...,\n",
      "         4.87731273e-31,  4.43061641e-31,  4.84048167e-31],\n",
      "       [-9.40254256e-02, -9.67785567e-02,  1.49484321e-01, ...,\n",
      "         4.37539071e-02,  5.03440872e-02, -2.86789000e-01],\n",
      "       [ 1.85456321e-01, -1.97804496e-01,  8.83101020e-03, ...,\n",
      "         2.11668208e-01, -1.69330597e-01, -5.06321453e-02],\n",
      "       ...,\n",
      "       [-1.51500508e-01,  2.61624064e-03, -8.34051445e-02, ...,\n",
      "         1.22352289e-02, -2.16690615e-01, -2.70119876e-01],\n",
      "       [-6.12209700e-02, -3.10104668e-01,  1.49566785e-01, ...,\n",
      "        -5.48524298e-02,  1.23864405e-01,  1.07210651e-01],\n",
      "       [ 2.78939635e-01, -7.22487420e-02,  1.71503186e-01, ...,\n",
      "        -3.71317528e-02, -2.76982844e-01,  7.50769451e-02]], dtype=float32), array([-0.01840414, -0.11669148, -0.01995228, -0.01095229, -0.00658043,\n",
      "       -0.00325373, -0.07418321, -0.06918389,  0.17763102,  0.04337242],\n",
      "      dtype=float32)]\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[array([[-1.7844683e-32,  1.3623944e-32, -2.4625073e-32, ...,\n",
      "        -9.0704465e-33,  5.7878127e-32, -4.8722243e-32],\n",
      "       [ 2.9494564e-32,  1.8965859e-33,  6.8861848e-33, ...,\n",
      "        -3.9435720e-32,  3.1167328e-32,  5.0216619e-33],\n",
      "       [ 5.5530982e-32,  5.5159943e-32,  3.2902910e-33, ...,\n",
      "        -4.8895792e-32, -2.4721804e-32, -4.4585713e-32],\n",
      "       ...,\n",
      "       [-4.5627166e-33, -3.5246290e-32, -2.3581713e-32, ...,\n",
      "        -4.7059988e-32, -3.5589055e-32,  4.9196449e-32],\n",
      "       [-1.3099095e-32, -3.1499416e-32,  1.3375104e-32, ...,\n",
      "        -4.6898945e-32,  4.7419871e-32, -4.7809824e-33],\n",
      "       [-4.6035533e-33, -3.6502626e-33, -6.8788680e-34, ...,\n",
      "        -1.6749020e-32,  2.6938310e-32, -4.5302153e-32]], dtype=float32), array([ 4.41116244e-02, -2.20513046e-02, -2.29105316e-02, -3.01714372e-02,\n",
      "        9.57919098e-03, -8.95479396e-02,  2.29536798e-02,  4.95737605e-02,\n",
      "       -1.30351987e-02, -3.50225531e-02,  6.55911639e-02, -1.57437976e-02,\n",
      "       -3.59149054e-02, -5.23337722e-02,  5.91720007e-02, -8.53534974e-03,\n",
      "        2.16392875e-02,  5.08685037e-03, -4.02847342e-02, -4.97057987e-03,\n",
      "       -8.83265212e-02, -4.57055867e-03, -6.81671426e-02,  1.27231956e-01,\n",
      "        3.02412212e-02,  2.24127583e-02, -5.79890683e-02,  2.95017492e-02,\n",
      "       -1.30201327e-02, -1.28436806e-02,  1.25954933e-02,  2.89599355e-02,\n",
      "        7.56247900e-03, -1.28780037e-01, -1.32086396e-01,  8.72921646e-02,\n",
      "       -1.44768432e-02, -3.53562683e-02,  8.56259614e-02,  1.61070470e-02,\n",
      "       -1.44630019e-02, -2.28572562e-02, -2.22460292e-02, -4.44116294e-02,\n",
      "       -2.66413651e-02,  3.77606452e-02,  3.90675552e-02, -1.98412109e-02,\n",
      "       -8.50456133e-02,  2.24378258e-02,  4.26793331e-03, -2.41539571e-02,\n",
      "       -9.18617938e-03, -1.20685035e-02, -5.44183403e-02,  7.29259625e-02,\n",
      "       -4.62926216e-02, -2.43727285e-02, -9.95587185e-03, -1.89634156e-03,\n",
      "        3.46820764e-02,  7.39797354e-02, -9.74316448e-02, -1.51490634e-02,\n",
      "       -1.50165493e-02, -5.24311513e-02,  4.61248728e-03,  3.71900834e-02,\n",
      "       -4.99547347e-02, -1.67943295e-02,  7.38122761e-02,  3.51653025e-02,\n",
      "       -1.54415462e-02, -1.29496958e-02, -5.57271205e-02, -1.83183309e-02,\n",
      "        4.48527224e-02,  4.67452370e-02,  5.14115468e-02, -4.33305884e-03,\n",
      "       -9.31677874e-03,  2.59251781e-02, -5.71389906e-02,  1.62688587e-02,\n",
      "        1.48283439e-02,  2.01325188e-03,  9.84209962e-03,  1.48799300e-01,\n",
      "       -1.09781651e-02, -1.61458552e-02, -7.36314012e-03,  7.15116784e-03,\n",
      "       -1.08879374e-03, -4.52603139e-02,  6.50206059e-02,  2.49094293e-02,\n",
      "        1.87849645e-02,  8.93601626e-02,  3.93521003e-02, -4.14865613e-02,\n",
      "        5.98873347e-02, -3.40173654e-02,  1.69799905e-02, -4.65587713e-02,\n",
      "        5.43219261e-02, -1.84993222e-02,  1.05425298e-01,  6.60060858e-03,\n",
      "       -2.59678462e-04, -1.22658825e-02, -2.65405942e-02,  8.81480053e-02,\n",
      "       -3.17971371e-02, -2.69421400e-03,  4.02777866e-02,  5.94804883e-02,\n",
      "        2.96249315e-02, -1.98287517e-02, -9.65998322e-02, -4.00327370e-02,\n",
      "       -1.08237145e-02,  2.48760413e-02, -4.01539030e-03, -2.38805515e-04,\n",
      "        4.75521162e-02, -2.74467729e-02,  8.28831792e-02,  5.56050539e-02,\n",
      "        8.72438103e-02, -1.48192002e-02, -1.44244656e-02,  4.94377278e-02,\n",
      "        5.28288186e-02,  5.07488772e-02,  3.87453027e-02, -2.47374345e-02,\n",
      "       -7.37827495e-02,  4.34840582e-02,  3.70356776e-02,  2.60882713e-02,\n",
      "        5.02078496e-02, -4.86939866e-03, -4.01832312e-02,  6.59602806e-02,\n",
      "        4.10662666e-02,  9.17150639e-03, -2.42466237e-02, -2.67401468e-02,\n",
      "       -5.97764214e-04,  7.10305339e-03,  1.60129238e-02, -2.15408336e-02,\n",
      "        6.86729029e-02,  2.43976479e-03, -3.00747100e-02, -4.77475673e-03,\n",
      "       -9.61354189e-03, -4.52234857e-02,  1.39340937e-01,  2.93087680e-03,\n",
      "        2.21187267e-02,  1.70205496e-02,  7.35703632e-02, -1.92328133e-02,\n",
      "       -3.58944014e-02, -1.68813672e-02,  1.96955986e-02,  4.73264456e-02,\n",
      "       -2.21782718e-02, -4.95152967e-03,  1.16515741e-01, -2.54204404e-02,\n",
      "       -4.18819971e-02,  2.58312896e-02,  1.26664847e-01,  5.62267238e-03,\n",
      "        7.76086301e-02,  2.50084680e-02,  1.42398970e-02,  7.74948075e-02,\n",
      "        4.90757972e-02, -2.83874776e-02,  3.15661803e-02,  3.52093764e-02,\n",
      "       -4.29430753e-02, -2.80730147e-02, -8.90930817e-02, -1.54408338e-02,\n",
      "       -7.75257573e-02, -4.65556368e-05,  4.76387404e-02,  4.93010767e-02,\n",
      "       -6.08423464e-02, -6.14856370e-03, -9.09451842e-02,  8.85374993e-02,\n",
      "        2.56177550e-03,  5.66253625e-02,  1.87379401e-02, -2.83822976e-02,\n",
      "        3.19507867e-02,  4.07592952e-02, -6.73909578e-03,  4.18129563e-02,\n",
      "       -5.79973171e-03, -1.30386523e-03, -4.70712371e-02,  1.41945668e-02,\n",
      "       -4.89693275e-03,  9.99397710e-02, -2.34237090e-02,  4.56329137e-02,\n",
      "        6.47003576e-02, -1.96036212e-02,  2.48392746e-02,  3.56042981e-02,\n",
      "       -1.03055139e-03,  1.33999735e-01,  5.39684892e-02, -2.92906072e-02,\n",
      "       -6.61796853e-02, -7.32746795e-02,  6.27827048e-02,  2.56896764e-02,\n",
      "        5.48328944e-02,  1.60031449e-02, -1.11514153e-02,  3.93619761e-02,\n",
      "        3.35754640e-02, -1.62827596e-02, -4.80255596e-02,  3.87622118e-02,\n",
      "        6.92419708e-02, -6.95667043e-03, -2.74305418e-02, -6.58746734e-02,\n",
      "       -2.14973260e-02,  7.08335266e-03, -1.92379970e-02,  2.03654580e-02,\n",
      "        2.32839044e-02,  2.05007736e-02, -4.87987176e-02, -4.75638692e-04,\n",
      "        7.49004409e-02,  1.74893960e-02, -1.52621733e-03, -3.02197002e-02,\n",
      "       -1.03032909e-01, -8.25000554e-02,  1.05432354e-01,  1.67104546e-02,\n",
      "        6.88468888e-02,  2.40296610e-02,  9.87408310e-02,  5.03101759e-03],\n",
      "      dtype=float32), array([[-3.8074231e-31,  3.6164004e-02, -3.7465550e-03, ...,\n",
      "         2.9137393e-02, -4.1835845e-02, -1.9035531e-02],\n",
      "       [ 4.9093012e-31, -3.2019012e-02,  9.0279700e-03, ...,\n",
      "        -7.6891497e-02,  4.6194280e-03,  2.3395095e-02],\n",
      "       [-3.8965235e-31, -5.5067915e-02,  5.0139152e-02, ...,\n",
      "        -3.6282785e-02,  1.9902220e-02,  7.4532561e-02],\n",
      "       ...,\n",
      "       [-3.8922588e-31,  1.1348985e-02, -1.5724810e-02, ...,\n",
      "         2.0718284e-02,  3.4585107e-02, -4.4128291e-02],\n",
      "       [-4.5730459e-31,  9.7271644e-02, -4.9753856e-02, ...,\n",
      "         1.1498870e-01, -4.0233484e-03,  7.0252810e-03],\n",
      "       [-4.8305666e-31, -3.9849401e-02,  2.5747441e-02, ...,\n",
      "        -1.5135340e-02,  6.5027624e-02,  2.3806401e-02]], dtype=float32), array([-8.14932361e-02,  4.74461960e-03, -1.57375764e-02,  5.75402714e-02,\n",
      "        1.56591043e-01, -3.24418657e-02,  3.43626216e-02, -1.53806582e-02,\n",
      "       -2.37967484e-02,  9.46400911e-02, -8.82545561e-02, -6.36462774e-03,\n",
      "        7.97297060e-02,  3.94200459e-02, -1.12368660e-02,  6.02367055e-03,\n",
      "        1.56496897e-01,  1.96969613e-01, -8.86230264e-03,  1.43265158e-01,\n",
      "        7.34163856e-04,  8.10091868e-02,  8.41038376e-02,  5.63556142e-02,\n",
      "       -2.51622088e-02,  6.27735630e-02,  2.54746750e-02,  2.83681229e-02,\n",
      "       -5.60860299e-02, -4.54483442e-02,  1.42771423e-01,  1.67926461e-01,\n",
      "        1.10653289e-01,  1.84535682e-01,  9.58856121e-02,  1.31630361e-01,\n",
      "        6.37976453e-02,  4.56525087e-02,  1.24812303e-02,  1.30016282e-01,\n",
      "       -2.19346993e-02,  1.03227451e-01,  9.06669945e-02,  9.08903256e-02,\n",
      "       -4.83336626e-03,  3.35617624e-02,  9.25840214e-02,  1.58889242e-03,\n",
      "       -5.48756160e-02,  1.50993809e-01,  1.67745441e-01,  9.04000849e-02,\n",
      "        3.38690472e-03,  1.57068342e-01,  4.27833982e-02,  7.09905922e-02,\n",
      "       -2.75579896e-02,  1.06946968e-01, -4.37989309e-02,  3.09817772e-02,\n",
      "        1.53020009e-01, -5.40502090e-03,  9.26020443e-02, -3.24947871e-02,\n",
      "        6.79544881e-02,  3.49818292e-04,  7.18823029e-03,  1.74483687e-01,\n",
      "        4.02093818e-03,  2.82175802e-02,  2.59699933e-02,  1.11287400e-01,\n",
      "        1.59175038e-01, -4.78900820e-02, -2.22665519e-02, -3.13159497e-03,\n",
      "        7.17234192e-03,  7.58898333e-02, -5.16683757e-02,  1.51841026e-02,\n",
      "       -2.30279211e-02,  9.86637399e-02,  1.11019894e-01,  1.31028831e-01,\n",
      "        1.14065278e-02,  1.61418602e-01,  5.15416935e-02, -6.01639226e-02,\n",
      "        6.31287917e-02,  2.13980302e-02,  1.38996527e-01, -1.82353165e-02,\n",
      "        1.85423017e-01, -9.43167694e-03,  1.28725454e-01,  8.86330083e-02,\n",
      "       -1.63685698e-02,  1.24633417e-01,  9.94081125e-02,  1.52303293e-01,\n",
      "        2.02982366e-01,  9.09324661e-02,  1.24291018e-01,  8.41214023e-06,\n",
      "        7.66754821e-02,  5.04045747e-02,  5.64191630e-03, -5.75296879e-02,\n",
      "       -1.65306702e-02,  8.18219185e-02,  6.66683316e-02,  2.13128533e-02,\n",
      "       -4.91340607e-02, -7.65508115e-02,  2.27158889e-02,  4.59107906e-02,\n",
      "        1.14315987e-01,  5.40230833e-02,  4.42199782e-02,  1.56954989e-01,\n",
      "        8.97869691e-02,  2.94518918e-02,  4.71440405e-02, -3.19887772e-02,\n",
      "       -6.98238984e-02, -2.83639859e-02,  1.51739538e-01, -6.58504814e-02,\n",
      "       -1.45611642e-02, -1.65885054e-02,  1.00904875e-01, -8.82754847e-03,\n",
      "       -7.99267590e-02,  2.93533318e-02,  1.47569314e-01,  1.12509772e-01,\n",
      "        1.34589849e-02, -6.31046817e-02,  2.58851778e-02,  5.49162040e-03,\n",
      "        6.36460185e-02, -6.60989759e-03,  1.85025647e-01,  1.51599973e-01,\n",
      "       -3.63826044e-02,  5.05557060e-02,  4.15443517e-02,  2.87578232e-03,\n",
      "       -8.72953515e-03,  1.10034183e-01, -3.79142836e-02,  7.23893642e-02,\n",
      "       -6.64181188e-02,  1.09862380e-01,  3.85375582e-02,  6.75506368e-02,\n",
      "        1.58621877e-01, -5.64608723e-02,  1.82081416e-01, -4.99807559e-02,\n",
      "        2.34852526e-02, -4.01211940e-02,  9.96668562e-02,  9.48135406e-02,\n",
      "       -5.17400503e-02, -9.22965705e-02, -2.24165022e-02, -1.05068097e-02,\n",
      "        2.31061317e-03,  1.03694117e-02,  8.37301910e-02,  5.30177392e-02,\n",
      "        5.32704815e-02,  6.21215403e-02,  7.71790668e-02, -1.25054875e-02,\n",
      "        1.52298152e-01,  6.90092817e-02, -7.52527639e-02, -6.74139149e-03,\n",
      "        1.95869375e-02,  9.59277898e-03, -3.69702466e-02, -5.17258607e-02,\n",
      "        1.77846514e-02, -2.70704404e-02,  8.31003413e-02,  3.19901183e-02,\n",
      "        4.80074883e-02, -3.30440849e-02, -3.56207825e-02, -7.83505011e-03,\n",
      "        1.87132746e-01, -6.81807622e-02,  7.48498812e-02,  1.77136406e-01,\n",
      "        2.80063152e-02,  1.87921580e-02,  1.21084586e-01,  1.04545623e-01,\n",
      "        1.94921643e-02,  1.95906967e-01, -4.01193183e-03,  9.18128118e-02,\n",
      "        4.43472452e-02,  1.05904356e-01,  3.98264220e-03, -1.97361391e-02,\n",
      "       -2.51582563e-02,  6.25557527e-02,  1.30449384e-01, -7.74362758e-02,\n",
      "       -3.20490785e-02,  8.56067389e-02,  1.80949003e-01, -5.05657084e-02,\n",
      "       -1.81537662e-02,  1.31997004e-01,  5.56533644e-03,  1.65286735e-02,\n",
      "       -3.40857320e-02,  2.02900931e-01, -1.40265608e-02,  7.60712847e-02,\n",
      "        2.53461245e-02,  8.59234780e-02,  1.61580276e-02,  1.10857859e-02,\n",
      "       -4.44391323e-03,  3.66279259e-02,  8.38291273e-02,  7.65340179e-02,\n",
      "        9.00034606e-02,  5.18020317e-02,  7.79718626e-03,  7.96637684e-02,\n",
      "       -2.03274097e-02,  1.23810470e-01,  2.45951880e-02, -8.46773945e-03,\n",
      "        1.31936520e-01, -9.58304759e-03,  1.63948044e-01,  1.42533928e-01,\n",
      "        7.14992285e-02,  1.32686168e-01,  1.79248378e-02, -3.55118848e-02,\n",
      "       -5.70958294e-02,  1.12110689e-01,  1.38645172e-01,  1.29469680e-02,\n",
      "       -1.94553472e-02,  2.28653643e-02,  1.64320797e-01, -3.22366890e-04],\n",
      "      dtype=float32), array([[-3.94860448e-31,  5.23516848e-31, -4.81263233e-31, ...,\n",
      "         4.87731273e-31,  4.43061641e-31,  4.84048167e-31],\n",
      "       [-9.40254256e-02, -9.67785567e-02,  1.49484321e-01, ...,\n",
      "         4.37539071e-02,  5.03440872e-02, -2.86789000e-01],\n",
      "       [ 1.85456321e-01, -1.97804496e-01,  8.83101020e-03, ...,\n",
      "         2.11668208e-01, -1.69330597e-01, -5.06321453e-02],\n",
      "       ...,\n",
      "       [-1.51500508e-01,  2.61624064e-03, -8.34051445e-02, ...,\n",
      "         1.22352289e-02, -2.16690615e-01, -2.70119876e-01],\n",
      "       [-6.12209700e-02, -3.10104668e-01,  1.49566785e-01, ...,\n",
      "        -5.48524298e-02,  1.23864405e-01,  1.07210651e-01],\n",
      "       [ 2.78939635e-01, -7.22487420e-02,  1.71503186e-01, ...,\n",
      "        -3.71317528e-02, -2.76982844e-01,  7.50769451e-02]], dtype=float32), array([-0.01840414, -0.11669148, -0.01995228, -0.01095229, -0.00658043,\n",
      "       -0.00325373, -0.07418321, -0.06918389,  0.17763102,  0.04337242],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    network = load_model('network1.h5')\n",
    "    batch_size = 32\n",
    "    epochs = 1000\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "                validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "                patience=2, learning_rate_decay=True, alpha=0.001)\n",
    "    save_model(network, 'network2.h5')\n",
    "    network.summary()\n",
    "    print(network.get_weights())\n",
    "    del network\n",
    "\n",
    "    network2 = load_model('network2.h5')\n",
    "    network2.summary()\n",
    "    print(network2.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dbeab254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(network, data, labels, verbose=True):\n",
    "    \"\"\"Test model.\"\"\"\n",
    "    result = network.evaluate(data, labels, verbose=verbose)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e2ac540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 0.1013 - accuracy: 0.9831\n",
      "[0.10127848386764526, 0.9830999970436096]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_test = datasets['X_test']\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    Y_test = datasets['Y_test']\n",
    "    Y_test_oh = one_hot(Y_test)\n",
    "\n",
    "    network = load_model('network1.h5')\n",
    "    print(test_model(network, X_test, Y_test_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b352a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, data, verbose=False):\n",
    "    \"\"\"Make prediction.\"\"\"\n",
    "    prediction = network.predict(data, verbose=verbose)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1fd9d449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.3719714e-07 5.4780651e-07 1.4773923e-05 ... 9.9989641e-01\n",
      "  6.9536975e-07 1.1765170e-05]\n",
      " [3.9417483e-08 3.3900873e-05 9.9995100e-01 ... 4.2623856e-09\n",
      "  1.0504880e-06 7.9508747e-13]\n",
      " [3.9598663e-06 9.9860507e-01 9.3497118e-05 ... 5.6158868e-04\n",
      "  5.5966200e-04 5.2904920e-07]\n",
      " ...\n",
      " [3.8534544e-12 2.0532471e-09 1.1840533e-11 ... 3.3902825e-07\n",
      "  9.8977182e-09 1.5435379e-06]\n",
      " [8.0749821e-08 3.7733518e-08 1.0712753e-09 ... 1.4866238e-07\n",
      "  2.7013547e-04 1.9977902e-09]\n",
      " [7.9525972e-07 1.5763383e-09 8.7963537e-07 ... 1.4232055e-11\n",
      "  1.8371952e-07 3.4404530e-09]]\n",
      "[7 2 1 ... 4 5 6]\n",
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_test = datasets['X_test']\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "Y_test = datasets['Y_test']\n",
    "\n",
    "network = load_model('network1.h5')\n",
    "Y_pred = predict(network, X_test)\n",
    "print(Y_pred)\n",
    "print(np.argmax(Y_pred, axis=1))\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2fec539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_config(network, filename):\n",
    "    \"\"\"Save model.\"\"\"\n",
    "    with open(filename, 'w') as modeljson:\n",
    "        modeljson.write(network.to_json())\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_config(filename):\n",
    "    \"\"\"Load model.\"\"\"\n",
    "    with open(filename, 'r') as modeljson:\n",
    "        model = K.models.model_from_json(modeljson.read())\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92f9c280",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [57], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m save_config(network, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig1.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m network\n\u001b[0;32m---> 20\u001b[0m network2 \u001b[38;5;241m=\u001b[39m \u001b[43mload_config\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig1.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m network2\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(network2\u001b[38;5;241m.\u001b[39mget_weights())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_config' is not defined"
     ]
    }
   ],
   "source": [
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    network = load_model('network1.h5')\n",
    "    save_config(network, 'config1.json')\n",
    "    del network\n",
    "\n",
    "    network2 = load_config('config1.json')\n",
    "    network2.summary()\n",
    "    print(network2.get_weights())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('holbertonschool-machine_learning-zF0IEfUY')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1bb7f88ff86f8a65293f6958f33bcf6ed9c9a574270708a89302acb2663e3cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
