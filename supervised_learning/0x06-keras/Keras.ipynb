{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a85fa9ba-ce1f-492e-b2af-e3f736417f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 18:38:47.906623: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-13 18:38:47.906641: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12f234a7-2831-4d01-9541-18e5d7ac9c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"\n",
    "    Build a neural network with the Keras library\n",
    "\n",
    "    nx is the number of input features to the network\n",
    "\n",
    "    layers is a list containing the number of nodes\n",
    "    in each layer of the network\n",
    "\n",
    "    activations is a list containing the activation\n",
    "\n",
    "    functions used for each layer of the network\n",
    "\n",
    "    lambtha is the L2 regularization parameter\n",
    "\n",
    "    keep_prob is the probability that a node will\n",
    "    be kept for dropout\n",
    "\n",
    "    You are not allowed to use the Input class\n",
    "\n",
    "    Returns: the keras model\n",
    "    \"\"\"\n",
    "    model = K.Sequential()\n",
    "    for i, layer in enumerate(layers):\n",
    "        l2 = K.regularizers.L2(lambtha)\n",
    "        if i == 0:\n",
    "            model.add(K.layers.Dense(layer, input_shape=(nx,),\n",
    "                                     activation=activations[i],\n",
    "                                     kernel_regularizer=l2))\n",
    "        else:\n",
    "            model.add(K.layers.Dense(layer, \n",
    "                                     activation=activations[i],\n",
    "                                     kernel_regularizer=l2))\n",
    "        if i != len(layers)-1:\n",
    "            model.add(K.layers.Dropout(1-keep_prob))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd64eda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.38640815>, <tf.Tensor: shape=(), dtype=float32, numpy=0.25695384>, <tf.Tensor: shape=(), dtype=float32, numpy=0.019414766>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 18:38:58.414279: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-13 18:38:58.414316: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-13 18:38:58.414347: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (archpc): /proc/driver/nvidia/version does not exist\n",
      "2022-09-13 18:38:58.414648: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    network = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    network.summary()\n",
    "    print(network.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db066013",
   "metadata": {},
   "source": [
    "# You are not allowed to use the Sequential class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff137fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"\n",
    "    Build a neural network with the Keras library\n",
    "\n",
    "    nx is the number of input features to the network\n",
    "\n",
    "    layers is a list containing the number of nodes in\n",
    "    each layer of the network\n",
    "\n",
    "    activations is a list containing the activation functions\n",
    "    used for each layer of the network\n",
    "\n",
    "    lambtha is the L2 regularization parameter\n",
    "\n",
    "    keep_prob is the probability that a node will be kept for dropout\n",
    "    Returns: the keras model\n",
    "    \"\"\"\n",
    "    prev = K.Input(shape=(nx,))\n",
    "    inputs = prev\n",
    "    l2 = K.regularizers.L2(lambtha)\n",
    "    for i, layer in enumerate(layers):\n",
    "        prev = K.layers.Dense(layer, activation=activations[i], kernel_regularizer=l2)(prev)\n",
    "        if i != len(layers) - 1:\n",
    "            prev = K.layers.Dropout(keep_prob)(prev)\n",
    "\n",
    "    model = K.Model(inputs=inputs, outputs=prev)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69007df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 19:42:56.579719: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-13 19:42:56.579739: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.38640815>, <tf.Tensor: shape=(), dtype=float32, numpy=0.25695384>, <tf.Tensor: shape=(), dtype=float32, numpy=0.019414766>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 19:42:57.975802: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-13 19:42:57.975827: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-13 19:42:57.975852: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (archpc): /proc/driver/nvidia/version does not exist\n",
      "2022-09-13 19:42:57.976057: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    network = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    network.summary()\n",
    "    print(network.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "258a4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(network, alpha, beta1, beta2):\n",
    "    \"\"\"\n",
    "    sets up Adam optimization for a keras model with\\\n",
    "    categorical crossentropy loss and accuracy metrics.\n",
    "\n",
    "    network is the model to optimize\n",
    "    alpha is the learning rate\n",
    "    beta1 is the first Adam optimization parameter\n",
    "    beta2 is the second Adam optimization parameter\n",
    "    Returns: None \n",
    "    \"\"\"\n",
    "    optimizer = K.optimizers.Adam(alpha, beta1, beta2)\n",
    "    network.compile(optimizer=optimizer,\n",
    "                    loss=K.losses.CategoricalCrossentropy(),\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fd72b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.losses.CategoricalCrossentropy'>\n",
      "<class 'keras.optimizer_v2.adam.Adam'>\n",
      "(0.01, 0.99, 0.9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    optimize_model(model, 0.01, 0.99, 0.9)\n",
    "    print(model.loss)\n",
    "    opt = model.optimizer\n",
    "    print(opt.__class__)\n",
    "    print(tuple(map(lambda x: x.numpy(),(opt.lr, opt.beta_1, opt.beta_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "475ef9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, classes=None):\n",
    "    \"\"\"One hot encoding keras.\"\"\"\n",
    "    if classes is None:\n",
    "        classes = max(labels) + 1\n",
    "    layer = K.layers.CategoryEncoding(\n",
    "        num_tokens=classes, output_mode=\"one_hot\")\n",
    "    return layer(labels).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59be1a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    labels = np.load('../data/MNIST.npz')['Y_train'][:10]\n",
    "    print(labels)\n",
    "    print(one_hot(labels))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edafce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    Train Keras Model.\n",
    "\n",
    "    network is the model to train\n",
    "\n",
    "    data is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "\n",
    "    labels is a one-hot numpy.ndarray of shape (m, classes)\n",
    "     containing the labels of data\n",
    "\n",
    "    batch_size is the size of the batch used for mini-batch gradient descent\n",
    "\n",
    "    epochs is the number of passes through data for mini-batch gradient descent\n",
    "\n",
    "    verbose is a boolean that determines if output should be\n",
    "    printed during training\n",
    "\n",
    "    shuffle is a boolean that determines whether to\n",
    "    shuffle the batches every epoch.\n",
    "\n",
    "    Normally, it is a good idea to shuffle, but for reproducibility,\n",
    "    we have chosen to set the default to False.\n",
    "\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    history = network.fit(data, labels, batch_size, epochs,\n",
    "                          verbose=verbose, shuffle=shuffle)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f953ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 22:17:45.125496: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 156800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3256 - accuracy: 0.9210\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1748 - accuracy: 0.9658\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1409 - accuracy: 0.9754\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1249 - accuracy: 0.9803\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1147 - accuracy: 0.9839\n"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88f268bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                validation_data=None, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    Train Keras Model.\n",
    "\n",
    "    network is the model to train\n",
    "\n",
    "    data is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "\n",
    "    labels is a one-hot numpy.ndarray of shape (m, classes)\n",
    "     containing the labels of data\n",
    "\n",
    "    batch_size is the size of the batch used for mini-batch gradient descent\n",
    "\n",
    "    epochs is the number of passes through data for mini-batch gradient descent\n",
    "\n",
    "    verbose is a boolean that determines if output should be\n",
    "    printed during training\n",
    "\n",
    "    shuffle is a boolean that determines whether to\n",
    "    shuffle the batches every epoch.\n",
    "\n",
    "    Normally, it is a good idea to shuffle, but for reproducibility,\n",
    "    we have chosen to set the default to False.\n",
    "\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    history = network.fit(data, labels, batch_size, epochs,\n",
    "                          verbose=verbose, shuffle=shuffle,\n",
    "                          validation_data=validation_data)\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d85df1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 22:22:51.813670: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 156800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3256 - accuracy: 0.9210 - val_loss: 0.1889 - val_accuracy: 0.9628\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1748 - accuracy: 0.9658 - val_loss: 0.1586 - val_accuracy: 0.9700\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1409 - accuracy: 0.9754 - val_loss: 0.1555 - val_accuracy: 0.9724\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1249 - accuracy: 0.9803 - val_loss: 0.1529 - val_accuracy: 0.9738\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1147 - accuracy: 0.9839 - val_loss: 0.1510 - val_accuracy: 0.9748\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs, validation_data=(X_valid, Y_valid_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a13d810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network, data, labels, batch_size, epochs, \n",
    "                validation_data=None, early_stopping=False, \n",
    "                patience=0, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    Train Keras Model.\n",
    "\n",
    "    network is the model to train\n",
    "\n",
    "    data is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "\n",
    "    labels is a one-hot numpy.ndarray of shape (m, classes)\n",
    "     containing the labels of data\n",
    "\n",
    "    batch_size is the size of the batch used for mini-batch gradient descent\n",
    "\n",
    "    epochs is the number of passes through data for mini-batch gradient descent\n",
    "\n",
    "    verbose is a boolean that determines if output should be\n",
    "    printed during training\n",
    "\n",
    "    validation_data is the data to validate the model with, if not None\n",
    "\n",
    "    early_stopping is a boolean that indicates whether early stopping should be used\n",
    "\n",
    "    patience is the patience used for early stopping\n",
    "\n",
    "    shuffle is a boolean that determines whether to\n",
    "    shuffle the batches every epoch.\n",
    "\n",
    "    Normally, it is a good idea to shuffle, but for reproducibility,\n",
    "    we have chosen to set the default to False.\n",
    "\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    if validation_data:\n",
    "        early_stopping_callback = K.callbacks.EarlyStopping(\n",
    "            'val_loss', patience=patience)\n",
    "    history = network.fit(data, labels, batch_size, epochs,\n",
    "                          verbose=verbose, shuffle=shuffle,\n",
    "                          validation_data=validation_data,\n",
    "                          callbacks=early_stopping_callback)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40c5a660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3256 - accuracy: 0.9210 - val_loss: 0.1889 - val_accuracy: 0.9628\n",
      "Epoch 2/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1748 - accuracy: 0.9658 - val_loss: 0.1586 - val_accuracy: 0.9700\n",
      "Epoch 3/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1409 - accuracy: 0.9754 - val_loss: 0.1555 - val_accuracy: 0.9724\n",
      "Epoch 4/30\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1249 - accuracy: 0.9803 - val_loss: 0.1529 - val_accuracy: 0.9738\n",
      "Epoch 5/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1147 - accuracy: 0.9839 - val_loss: 0.1510 - val_accuracy: 0.9748\n",
      "Epoch 6/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1105 - accuracy: 0.9850 - val_loss: 0.1416 - val_accuracy: 0.9764\n",
      "Epoch 7/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1015 - accuracy: 0.9873 - val_loss: 0.1860 - val_accuracy: 0.9653\n",
      "Epoch 8/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1007 - accuracy: 0.9869 - val_loss: 0.1438 - val_accuracy: 0.9776\n",
      "Epoch 9/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0989 - accuracy: 0.9879 - val_loss: 0.1471 - val_accuracy: 0.9751\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 30\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "                validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "                patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd106298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                validation_data=None, early_stopping=False,\n",
    "                patience=0, learning_rate_decay=False,\n",
    "                alpha=0.1, decay_rate=1, verbose=True,\n",
    "                shuffle=False):\n",
    "    \"\"\"\n",
    "    Train Keras Model.\n",
    "\n",
    "    network is the model to train\n",
    "\n",
    "    data is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "\n",
    "    labels is a one-hot numpy.ndarray of shape (m, classes)\n",
    "     containing the labels of data\n",
    "\n",
    "    batch_size is the size of the batch used for mini-batch gradient descent\n",
    "\n",
    "    epochs is the number of passes through data for mini-batch gradient descent\n",
    "\n",
    "    verbose is a boolean that determines if output should be\n",
    "    printed during training\n",
    "\n",
    "    validation_data is the data to validate the model with, if not None\n",
    "\n",
    "    early_stopping is a boolean that indicates whether\n",
    "    early stopping should be used\n",
    "\n",
    "    patience is the patience used for early stopping\n",
    "\n",
    "    learning_rate_decay is a boolean that indicates whether learning rate\n",
    "    decay should be used\n",
    "\n",
    "    alpha is the initial learning rate\n",
    "\n",
    "    decay_rate is the decay rate\n",
    "\n",
    "    shuffle is a boolean that determines whether to\n",
    "    shuffle the batches every epoch.\n",
    "\n",
    "    Normally, it is a good idea to shuffle, but for reproducibility,\n",
    "    we have chosen to set the default to False.\n",
    "\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "\n",
    "    def schedule(epoch):\n",
    "        previous_lr = 1\n",
    "\n",
    "        def lr(epoch, start_lr, decay):\n",
    "            nonlocal previous_lr\n",
    "            previous_lr *= (start_lr / (1. + decay * epoch))\n",
    "            return previous_lr\n",
    "        return lr(epoch, alpha, decay_rate)\n",
    "\n",
    "    callbacks = []\n",
    "    if validation_data:\n",
    "        if early_stopping:\n",
    "            early_stopping_callback = K.callbacks.EarlyStopping(\n",
    "                'val_loss', patience=patience)\n",
    "            callbacks.append(early_stopping_callback)\n",
    "        if learning_rate_decay:\n",
    "            lr_callback = K.callbacks.LearningRateScheduler(\n",
    "                schedule, verbose=True)\n",
    "            callbacks.append(lr_callback)\n",
    "\n",
    "    history = network.fit(data, labels, batch_size, epochs,\n",
    "                          verbose=verbose, shuffle=shuffle,\n",
    "                          validation_data=validation_data,\n",
    "                          callbacks=callbacks)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62e099b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3256 - accuracy: 0.9210 - val_loss: 0.1889 - val_accuracy: 0.9628\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0005.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1596 - accuracy: 0.9698 - val_loss: 0.1473 - val_accuracy: 0.9727\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0003333333333333333.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1267 - accuracy: 0.9798 - val_loss: 0.1345 - val_accuracy: 0.9753\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.00025.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1097 - accuracy: 0.9847 - val_loss: 0.1275 - val_accuracy: 0.9765\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.0002.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0994 - accuracy: 0.9874 - val_loss: 0.1261 - val_accuracy: 0.9771\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.00016666666666666666.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0921 - accuracy: 0.9897 - val_loss: 0.1223 - val_accuracy: 0.9780\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.00014285714285714287.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0854 - accuracy: 0.9912 - val_loss: 0.1210 - val_accuracy: 0.9790\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.000125.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0809 - accuracy: 0.9924 - val_loss: 0.1203 - val_accuracy: 0.9795\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.00011111111111111112.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0783 - accuracy: 0.9933 - val_loss: 0.1175 - val_accuracy: 0.9798\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.0001.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0742 - accuracy: 0.9943 - val_loss: 0.1169 - val_accuracy: 0.9796\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 9.090909090909092e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0725 - accuracy: 0.9946 - val_loss: 0.1138 - val_accuracy: 0.9805\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 8.333333333333333e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0701 - accuracy: 0.9953 - val_loss: 0.1130 - val_accuracy: 0.9804\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 7.692307692307693e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0690 - accuracy: 0.9954 - val_loss: 0.1111 - val_accuracy: 0.9809\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 7.142857142857143e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0674 - accuracy: 0.9957 - val_loss: 0.1107 - val_accuracy: 0.9817\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 6.666666666666667e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0658 - accuracy: 0.9963 - val_loss: 0.1111 - val_accuracy: 0.9816\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 6.25e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0636 - accuracy: 0.9967 - val_loss: 0.1088 - val_accuracy: 0.9816\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 5.882352941176471e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0627 - accuracy: 0.9967 - val_loss: 0.1089 - val_accuracy: 0.9815\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 5.555555555555556e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0620 - accuracy: 0.9970 - val_loss: 0.1078 - val_accuracy: 0.9821\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 5.2631578947368424e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0613 - accuracy: 0.9970 - val_loss: 0.1087 - val_accuracy: 0.9821\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 5e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0601 - accuracy: 0.9974 - val_loss: 0.1077 - val_accuracy: 0.9822\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 4.761904761904762e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0591 - accuracy: 0.9974 - val_loss: 0.1063 - val_accuracy: 0.9824\n",
      "Epoch 22/1000\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 4.545454545454546e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0587 - accuracy: 0.9977 - val_loss: 0.1055 - val_accuracy: 0.9834\n",
      "Epoch 23/1000\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 4.347826086956522e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0579 - accuracy: 0.9976 - val_loss: 0.1060 - val_accuracy: 0.9827\n",
      "Epoch 24/1000\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 4.1666666666666665e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0573 - accuracy: 0.9979 - val_loss: 0.1060 - val_accuracy: 0.9828\n",
      "Epoch 25/1000\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 4e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0566 - accuracy: 0.9979 - val_loss: 0.1057 - val_accuracy: 0.9818\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 1000\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "                validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "                patience=3, learning_rate_decay=True, alpha=alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('holbertonschool-machine_learning-zF0IEfUY')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1bb7f88ff86f8a65293f6958f33bcf6ed9c9a574270708a89302acb2663e3cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
