{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a85fa9ba-ce1f-492e-b2af-e3f736417f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 18:38:47.906623: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-13 18:38:47.906641: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12f234a7-2831-4d01-9541-18e5d7ac9c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"\n",
    "    Build a neural network with the Keras library\n",
    "\n",
    "    nx is the number of input features to the network\n",
    "\n",
    "    layers is a list containing the number of nodes\n",
    "    in each layer of the network\n",
    "\n",
    "    activations is a list containing the activation\n",
    "\n",
    "    functions used for each layer of the network\n",
    "\n",
    "    lambtha is the L2 regularization parameter\n",
    "\n",
    "    keep_prob is the probability that a node will\n",
    "    be kept for dropout\n",
    "\n",
    "    You are not allowed to use the Input class\n",
    "\n",
    "    Returns: the keras model\n",
    "    \"\"\"\n",
    "    model = K.Sequential()\n",
    "    for i, layer in enumerate(layers):\n",
    "        l2 = K.regularizers.L2(lambtha)\n",
    "        if i == 0:\n",
    "            model.add(K.layers.Dense(layer, input_shape=(nx,),\n",
    "                                     activation=activations[i],\n",
    "                                     kernel_regularizer=l2))\n",
    "        else:\n",
    "            model.add(K.layers.Dense(layer, \n",
    "                                     activation=activations[i],\n",
    "                                     kernel_regularizer=l2))\n",
    "        if i != len(layers)-1:\n",
    "            model.add(K.layers.Dropout(1-keep_prob))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd64eda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.38640815>, <tf.Tensor: shape=(), dtype=float32, numpy=0.25695384>, <tf.Tensor: shape=(), dtype=float32, numpy=0.019414766>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 18:38:58.414279: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-13 18:38:58.414316: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-13 18:38:58.414347: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (archpc): /proc/driver/nvidia/version does not exist\n",
      "2022-09-13 18:38:58.414648: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    network = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    network.summary()\n",
    "    print(network.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db066013",
   "metadata": {},
   "source": [
    "# You are not allowed to use the Sequential class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff137fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"\n",
    "    Build a neural network with the Keras library\n",
    "\n",
    "    nx is the number of input features to the network\n",
    "\n",
    "    layers is a list containing the number of nodes in\n",
    "    each layer of the network\n",
    "\n",
    "    activations is a list containing the activation functions\n",
    "    used for each layer of the network\n",
    "\n",
    "    lambtha is the L2 regularization parameter\n",
    "\n",
    "    keep_prob is the probability that a node will be kept for dropout\n",
    "    Returns: the keras model\n",
    "    \"\"\"\n",
    "    prev = K.Input(shape=(nx,))\n",
    "    inputs = prev\n",
    "    l2 = K.regularizers.L2(lambtha)\n",
    "    for i, layer in enumerate(layers):\n",
    "        prev = K.layers.Dense(layer, activation=activations[i], kernel_regularizer=l2)(prev)\n",
    "        if i != len(layers) - 1:\n",
    "            prev = K.layers.Dropout(keep_prob)(prev)\n",
    "\n",
    "    model = K.Model(inputs=inputs, outputs=prev)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69007df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 19:42:56.579719: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-13 19:42:56.579739: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.38640815>, <tf.Tensor: shape=(), dtype=float32, numpy=0.25695384>, <tf.Tensor: shape=(), dtype=float32, numpy=0.019414766>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 19:42:57.975802: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-13 19:42:57.975827: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-13 19:42:57.975852: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (archpc): /proc/driver/nvidia/version does not exist\n",
      "2022-09-13 19:42:57.976057: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    network = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    network.summary()\n",
    "    print(network.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "258a4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(network, alpha, beta1, beta2):\n",
    "    \"\"\"\n",
    "    sets up Adam optimization for a keras model with\\\n",
    "    categorical crossentropy loss and accuracy metrics.\n",
    "\n",
    "    network is the model to optimize\n",
    "    alpha is the learning rate\n",
    "    beta1 is the first Adam optimization parameter\n",
    "    beta2 is the second Adam optimization parameter\n",
    "    Returns: None \n",
    "    \"\"\"\n",
    "    optimizer = K.optimizers.Adam(alpha, beta1, beta2)\n",
    "    network.compile(optimizer=optimizer,\n",
    "                    loss=K.losses.CategoricalCrossentropy(),\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fd72b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.losses.CategoricalCrossentropy'>\n",
      "<class 'keras.optimizer_v2.adam.Adam'>\n",
      "(0.01, 0.99, 0.9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    optimize_model(model, 0.01, 0.99, 0.9)\n",
    "    print(model.loss)\n",
    "    opt = model.optimizer\n",
    "    print(opt.__class__)\n",
    "    print(tuple(map(lambda x: x.numpy(),(opt.lr, opt.beta_1, opt.beta_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "475ef9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, classes=None):\n",
    "    \"\"\"One hot encoding keras.\"\"\"\n",
    "    if classes is None:\n",
    "        classes = max(labels) + 1\n",
    "    layer = K.layers.CategoryEncoding(\n",
    "        num_tokens=classes, output_mode=\"one_hot\")\n",
    "    return layer(labels).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59be1a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    labels = np.load('../data/MNIST.npz')['Y_train'][:10]\n",
    "    print(labels)\n",
    "    print(one_hot(labels))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edafce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    Train Keras Model.\n",
    "\n",
    "    network is the model to train\n",
    "\n",
    "    data is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "\n",
    "    labels is a one-hot numpy.ndarray of shape (m, classes)\n",
    "     containing the labels of data\n",
    "\n",
    "    batch_size is the size of the batch used for mini-batch gradient descent\n",
    "\n",
    "    epochs is the number of passes through data for mini-batch gradient descent\n",
    "\n",
    "    verbose is a boolean that determines if output should be\n",
    "    printed during training\n",
    "\n",
    "    shuffle is a boolean that determines whether to\n",
    "    shuffle the batches every epoch.\n",
    "\n",
    "    Normally, it is a good idea to shuffle, but for reproducibility,\n",
    "    we have chosen to set the default to False.\n",
    "\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    history = network.fit(data, labels, batch_size, epochs,\n",
    "                          verbose=verbose, shuffle=shuffle)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f953ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 22:17:45.125496: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 156800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3256 - accuracy: 0.9210\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1748 - accuracy: 0.9658\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1409 - accuracy: 0.9754\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1249 - accuracy: 0.9803\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1147 - accuracy: 0.9839\n"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88f268bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                validation_data=None, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    Train Keras Model.\n",
    "\n",
    "    network is the model to train\n",
    "\n",
    "    data is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "\n",
    "    labels is a one-hot numpy.ndarray of shape (m, classes)\n",
    "     containing the labels of data\n",
    "\n",
    "    batch_size is the size of the batch used for mini-batch gradient descent\n",
    "\n",
    "    epochs is the number of passes through data for mini-batch gradient descent\n",
    "\n",
    "    verbose is a boolean that determines if output should be\n",
    "    printed during training\n",
    "\n",
    "    shuffle is a boolean that determines whether to\n",
    "    shuffle the batches every epoch.\n",
    "\n",
    "    Normally, it is a good idea to shuffle, but for reproducibility,\n",
    "    we have chosen to set the default to False.\n",
    "\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    history = network.fit(data, labels, batch_size, epochs,\n",
    "                          verbose=verbose, shuffle=shuffle,\n",
    "                          validation_data=validation_data)\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d85df1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 22:22:51.813670: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 156800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3256 - accuracy: 0.9210 - val_loss: 0.1889 - val_accuracy: 0.9628\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1748 - accuracy: 0.9658 - val_loss: 0.1586 - val_accuracy: 0.9700\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1409 - accuracy: 0.9754 - val_loss: 0.1555 - val_accuracy: 0.9724\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1249 - accuracy: 0.9803 - val_loss: 0.1529 - val_accuracy: 0.9738\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1147 - accuracy: 0.9839 - val_loss: 0.1510 - val_accuracy: 0.9748\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs, validation_data=(X_valid, Y_valid_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a13d810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network, data, labels, batch_size, epochs, \n",
    "                validation_data=None, early_stopping=False, \n",
    "                patience=0, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    Train Keras Model.\n",
    "\n",
    "    network is the model to train\n",
    "\n",
    "    data is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "\n",
    "    labels is a one-hot numpy.ndarray of shape (m, classes)\n",
    "     containing the labels of data\n",
    "\n",
    "    batch_size is the size of the batch used for mini-batch gradient descent\n",
    "\n",
    "    epochs is the number of passes through data for mini-batch gradient descent\n",
    "\n",
    "    verbose is a boolean that determines if output should be\n",
    "    printed during training\n",
    "\n",
    "    validation_data is the data to validate the model with, if not None\n",
    "\n",
    "    early_stopping is a boolean that indicates whether early stopping should be used\n",
    "\n",
    "    patience is the patience used for early stopping\n",
    "\n",
    "    shuffle is a boolean that determines whether to\n",
    "    shuffle the batches every epoch.\n",
    "\n",
    "    Normally, it is a good idea to shuffle, but for reproducibility,\n",
    "    we have chosen to set the default to False.\n",
    "\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    if validation_data:\n",
    "        early_stopping_callback = K.callbacks.EarlyStopping(\n",
    "            'val_loss', patience=patience)\n",
    "    history = network.fit(data, labels, batch_size, epochs,\n",
    "                          verbose=verbose, shuffle=shuffle,\n",
    "                          validation_data=validation_data,\n",
    "                          callbacks=early_stopping_callback)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40c5a660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3256 - accuracy: 0.9210 - val_loss: 0.1889 - val_accuracy: 0.9628\n",
      "Epoch 2/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1748 - accuracy: 0.9658 - val_loss: 0.1586 - val_accuracy: 0.9700\n",
      "Epoch 3/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1409 - accuracy: 0.9754 - val_loss: 0.1555 - val_accuracy: 0.9724\n",
      "Epoch 4/30\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1249 - accuracy: 0.9803 - val_loss: 0.1529 - val_accuracy: 0.9738\n",
      "Epoch 5/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1147 - accuracy: 0.9839 - val_loss: 0.1510 - val_accuracy: 0.9748\n",
      "Epoch 6/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1105 - accuracy: 0.9850 - val_loss: 0.1416 - val_accuracy: 0.9764\n",
      "Epoch 7/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1015 - accuracy: 0.9873 - val_loss: 0.1860 - val_accuracy: 0.9653\n",
      "Epoch 8/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1007 - accuracy: 0.9869 - val_loss: 0.1438 - val_accuracy: 0.9776\n",
      "Epoch 9/30\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0989 - accuracy: 0.9879 - val_loss: 0.1471 - val_accuracy: 0.9751\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 30\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "                validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "                patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd106298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                validation_data=None, early_stopping=False,\n",
    "                patience=0, learning_rate_decay=False,\n",
    "                alpha=0.1, decay_rate=1, verbose=True,\n",
    "                shuffle=False):\n",
    "    \"\"\"\n",
    "    Train Keras Model.\n",
    "\n",
    "    network is the model to train\n",
    "\n",
    "    data is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "\n",
    "    labels is a one-hot numpy.ndarray of shape (m, classes)\n",
    "     containing the labels of data\n",
    "\n",
    "    batch_size is the size of the batch used for mini-batch gradient descent\n",
    "\n",
    "    epochs is the number of passes through data for mini-batch gradient descent\n",
    "\n",
    "    verbose is a boolean that determines if output should be\n",
    "    printed during training\n",
    "\n",
    "    validation_data is the data to validate the model with, if not None\n",
    "\n",
    "    early_stopping is a boolean that indicates whether\n",
    "    early stopping should be used\n",
    "\n",
    "    patience is the patience used for early stopping\n",
    "\n",
    "    learning_rate_decay is a boolean that indicates whether learning rate\n",
    "    decay should be used\n",
    "\n",
    "    alpha is the initial learning rate\n",
    "\n",
    "    decay_rate is the decay rate\n",
    "\n",
    "    shuffle is a boolean that determines whether to\n",
    "    shuffle the batches every epoch.\n",
    "\n",
    "    Normally, it is a good idea to shuffle, but for reproducibility,\n",
    "    we have chosen to set the default to False.\n",
    "\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "\n",
    "    def schedule(epoch):\n",
    "        previous_lr = 1\n",
    "\n",
    "        def lr(epoch, start_lr, decay):\n",
    "            nonlocal previous_lr\n",
    "            previous_lr *= (start_lr / (1. + decay * epoch))\n",
    "            return previous_lr\n",
    "        return lr(epoch, alpha, decay_rate)\n",
    "\n",
    "    callbacks = []\n",
    "    if validation_data:\n",
    "        if early_stopping:\n",
    "            early_stopping_callback = K.callbacks.EarlyStopping(\n",
    "                'val_loss', patience=patience)\n",
    "            callbacks.append(early_stopping_callback)\n",
    "        if learning_rate_decay:\n",
    "            lr_callback = K.callbacks.LearningRateScheduler(\n",
    "                schedule, verbose=True)\n",
    "            callbacks.append(lr_callback)\n",
    "\n",
    "    history = network.fit(data, labels, batch_size, epochs,\n",
    "                          verbose=verbose, shuffle=shuffle,\n",
    "                          validation_data=validation_data,\n",
    "                          callbacks=callbacks)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62e099b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3256 - accuracy: 0.9210 - val_loss: 0.1889 - val_accuracy: 0.9628\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0005.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1596 - accuracy: 0.9698 - val_loss: 0.1473 - val_accuracy: 0.9727\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0003333333333333333.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1267 - accuracy: 0.9798 - val_loss: 0.1345 - val_accuracy: 0.9753\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.00025.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1097 - accuracy: 0.9847 - val_loss: 0.1275 - val_accuracy: 0.9765\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.0002.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0994 - accuracy: 0.9874 - val_loss: 0.1261 - val_accuracy: 0.9771\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.00016666666666666666.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0921 - accuracy: 0.9897 - val_loss: 0.1223 - val_accuracy: 0.9780\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.00014285714285714287.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0854 - accuracy: 0.9912 - val_loss: 0.1210 - val_accuracy: 0.9790\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.000125.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0809 - accuracy: 0.9924 - val_loss: 0.1203 - val_accuracy: 0.9795\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.00011111111111111112.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0783 - accuracy: 0.9933 - val_loss: 0.1175 - val_accuracy: 0.9798\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.0001.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0742 - accuracy: 0.9943 - val_loss: 0.1169 - val_accuracy: 0.9796\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 9.090909090909092e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0725 - accuracy: 0.9946 - val_loss: 0.1138 - val_accuracy: 0.9805\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 8.333333333333333e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0701 - accuracy: 0.9953 - val_loss: 0.1130 - val_accuracy: 0.9804\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 7.692307692307693e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0690 - accuracy: 0.9954 - val_loss: 0.1111 - val_accuracy: 0.9809\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 7.142857142857143e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0674 - accuracy: 0.9957 - val_loss: 0.1107 - val_accuracy: 0.9817\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 6.666666666666667e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0658 - accuracy: 0.9963 - val_loss: 0.1111 - val_accuracy: 0.9816\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 6.25e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0636 - accuracy: 0.9967 - val_loss: 0.1088 - val_accuracy: 0.9816\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 5.882352941176471e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0627 - accuracy: 0.9967 - val_loss: 0.1089 - val_accuracy: 0.9815\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 5.555555555555556e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0620 - accuracy: 0.9970 - val_loss: 0.1078 - val_accuracy: 0.9821\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 5.2631578947368424e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0613 - accuracy: 0.9970 - val_loss: 0.1087 - val_accuracy: 0.9821\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 5e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0601 - accuracy: 0.9974 - val_loss: 0.1077 - val_accuracy: 0.9822\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 4.761904761904762e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0591 - accuracy: 0.9974 - val_loss: 0.1063 - val_accuracy: 0.9824\n",
      "Epoch 22/1000\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 4.545454545454546e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0587 - accuracy: 0.9977 - val_loss: 0.1055 - val_accuracy: 0.9834\n",
      "Epoch 23/1000\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 4.347826086956522e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0579 - accuracy: 0.9976 - val_loss: 0.1060 - val_accuracy: 0.9827\n",
      "Epoch 24/1000\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 4.1666666666666665e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0573 - accuracy: 0.9979 - val_loss: 0.1060 - val_accuracy: 0.9828\n",
      "Epoch 25/1000\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 4e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0566 - accuracy: 0.9979 - val_loss: 0.1057 - val_accuracy: 0.9818\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 1000\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "                validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "                patience=3, learning_rate_decay=True, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f3f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                validation_data=None, early_stopping=False,\n",
    "                patience=0, learning_rate_decay=False,\n",
    "                alpha=0.1, decay_rate=1, save_best=False,\n",
    "                filepath=None, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    Train Keras Model.\n",
    "\n",
    "    network is the model to train\n",
    "\n",
    "    data is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "\n",
    "    labels is a one-hot numpy.ndarray of shape (m, classes)\n",
    "     containing the labels of data\n",
    "\n",
    "    batch_size is the size of the batch used for mini-batch gradient descent\n",
    "\n",
    "    epochs is the number of passes through data for mini-batch gradient descent\n",
    "\n",
    "    verbose is a boolean that determines if output should be\n",
    "    printed during training\n",
    "\n",
    "    validation_data is the data to validate the model with, if not None\n",
    "\n",
    "    early_stopping is a boolean that indicates whether\n",
    "    early stopping should be used\n",
    "\n",
    "    patience is the patience used for early stopping\n",
    "\n",
    "    learning_rate_decay is a boolean that indicates whether learning rate\n",
    "    decay should be used\n",
    "\n",
    "    alpha is the initial learning rate\n",
    "\n",
    "    decay_rate is the decay rate\n",
    "\n",
    "    save_best is a boolean indicating whether to save the model\n",
    "    after each epoch if it is the best\n",
    "\n",
    "    filepath is the file path where the model should be saved\n",
    "\n",
    "    shuffle is a boolean that determines whether to\n",
    "    shuffle the batches every epoch.\n",
    "\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "\n",
    "    def schedule(epoch):\n",
    "        previous_lr = 1\n",
    "\n",
    "        def lr(epoch, start_lr, decay):\n",
    "            nonlocal previous_lr\n",
    "            previous_lr *= (start_lr / (1. + decay * epoch))\n",
    "            return previous_lr\n",
    "        return lr(epoch, alpha, decay_rate)\n",
    "\n",
    "    callbacks = []\n",
    "    if validation_data:\n",
    "        if early_stopping:\n",
    "            early_stopping_callback = K.callbacks.EarlyStopping(\n",
    "                'val_loss', patience=patience)\n",
    "            callbacks.append(early_stopping_callback)\n",
    "        if learning_rate_decay:\n",
    "            lr_callback = K.callbacks.LearningRateScheduler(\n",
    "                schedule, verbose=True)\n",
    "            callbacks.append(lr_callback)\n",
    "        checkpoint = K.callbacks.ModelCheckpoint(filepath=filepath,\n",
    "                                                 save_best_only=save_best)\n",
    "        callbacks.append(checkpoint)\n",
    "\n",
    "    history = network.fit(data, labels, batch_size, epochs,\n",
    "                          verbose=verbose, shuffle=shuffle,\n",
    "                          validation_data=validation_data,\n",
    "                          callbacks=callbacks)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "383583ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                validation_data=None, early_stopping=False,\n",
    "                patience=0, learning_rate_decay=False,\n",
    "                alpha=0.1, decay_rate=1, save_best=False,\n",
    "                filepath=None, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    Train Keras Model.\n",
    "\n",
    "    network is the model to train\n",
    "\n",
    "    data is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "\n",
    "    labels is a one-hot numpy.ndarray of shape (m, classes)\n",
    "     containing the labels of data\n",
    "\n",
    "    batch_size is the size of the batch used for mini-batch gradient descent\n",
    "\n",
    "    epochs is the number of passes through data for mini-batch gradient descent\n",
    "\n",
    "    verbose is a boolean that determines if output should be\n",
    "    printed during training\n",
    "\n",
    "    validation_data is the data to validate the model with, if not None\n",
    "\n",
    "    early_stopping is a boolean that indicates whether\n",
    "    early stopping should be used\n",
    "\n",
    "    patience is the patience used for early stopping\n",
    "\n",
    "    learning_rate_decay is a boolean that indicates whether learning rate\n",
    "    decay should be used\n",
    "\n",
    "    alpha is the initial learning rate\n",
    "\n",
    "    decay_rate is the decay rate\n",
    "\n",
    "    save_best is a boolean indicating whether to save the model\n",
    "    after each epoch if it is the best\n",
    "\n",
    "    filepath is the file path where the model should be saved\n",
    "\n",
    "    shuffle is a boolean that determines whether to\n",
    "    shuffle the batches every epoch.\n",
    "\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "\n",
    "    def schedule(epoch):\n",
    "        previous_lr = 1\n",
    "\n",
    "        def lr(epoch, start_lr, decay):\n",
    "            nonlocal previous_lr\n",
    "            previous_lr *= (start_lr / (1. + decay * epoch))\n",
    "            return previous_lr\n",
    "        return lr(epoch, alpha, decay_rate)\n",
    "\n",
    "    callbacks = []\n",
    "    if validation_data:\n",
    "        if early_stopping:\n",
    "            early_stopping_callback = K.callbacks.EarlyStopping(\n",
    "                'val_loss', patience=patience)\n",
    "            callbacks.append(early_stopping_callback)\n",
    "        if learning_rate_decay:\n",
    "            lr_callback = K.callbacks.LearningRateScheduler(\n",
    "                schedule, verbose=True)\n",
    "            callbacks.append(lr_callback)\n",
    "        checkpoint = K.callbacks.ModelCheckpoint(filepath=filepath,\n",
    "                                                 save_best_only=save_best)\n",
    "        callbacks.append(checkpoint)\n",
    "\n",
    "    history = network.fit(data, labels, batch_size, epochs,\n",
    "                          verbose=verbose, shuffle=shuffle,\n",
    "                          validation_data=validation_data,\n",
    "                          callbacks=callbacks)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2078bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3256 - accuracy: 0.9210 - val_loss: 0.1889 - val_accuracy: 0.9628\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0005.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1596 - accuracy: 0.9698 - val_loss: 0.1473 - val_accuracy: 0.9727\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0003333333333333333.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1267 - accuracy: 0.9798 - val_loss: 0.1345 - val_accuracy: 0.9753\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.00025.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1097 - accuracy: 0.9847 - val_loss: 0.1275 - val_accuracy: 0.9765\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.0002.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0994 - accuracy: 0.9874 - val_loss: 0.1261 - val_accuracy: 0.9771\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.00016666666666666666.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0921 - accuracy: 0.9897 - val_loss: 0.1223 - val_accuracy: 0.9780\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.00014285714285714287.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0854 - accuracy: 0.9912 - val_loss: 0.1210 - val_accuracy: 0.9790\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.000125.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0809 - accuracy: 0.9924 - val_loss: 0.1203 - val_accuracy: 0.9795\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.00011111111111111112.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0783 - accuracy: 0.9933 - val_loss: 0.1175 - val_accuracy: 0.9798\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.0001.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0742 - accuracy: 0.9943 - val_loss: 0.1169 - val_accuracy: 0.9796\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 9.090909090909092e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0725 - accuracy: 0.9946 - val_loss: 0.1138 - val_accuracy: 0.9805\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 8.333333333333333e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0701 - accuracy: 0.9953 - val_loss: 0.1130 - val_accuracy: 0.9804\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 7.692307692307693e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0690 - accuracy: 0.9954 - val_loss: 0.1111 - val_accuracy: 0.9809\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 7.142857142857143e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0674 - accuracy: 0.9957 - val_loss: 0.1107 - val_accuracy: 0.9817\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 6.666666666666667e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0658 - accuracy: 0.9963 - val_loss: 0.1111 - val_accuracy: 0.9816\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 6.25e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0636 - accuracy: 0.9967 - val_loss: 0.1088 - val_accuracy: 0.9816\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 5.882352941176471e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0627 - accuracy: 0.9967 - val_loss: 0.1089 - val_accuracy: 0.9815\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 5.555555555555556e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0620 - accuracy: 0.9970 - val_loss: 0.1078 - val_accuracy: 0.9821\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 5.2631578947368424e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0613 - accuracy: 0.9970 - val_loss: 0.1087 - val_accuracy: 0.9821\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 5e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0601 - accuracy: 0.9974 - val_loss: 0.1077 - val_accuracy: 0.9822\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 4.761904761904762e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0591 - accuracy: 0.9974 - val_loss: 0.1063 - val_accuracy: 0.9824\n",
      "Epoch 22/1000\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 4.545454545454546e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0587 - accuracy: 0.9977 - val_loss: 0.1055 - val_accuracy: 0.9834\n",
      "Epoch 23/1000\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 4.347826086956522e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0579 - accuracy: 0.9976 - val_loss: 0.1060 - val_accuracy: 0.9827\n",
      "Epoch 24/1000\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 4.1666666666666665e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0573 - accuracy: 0.9979 - val_loss: 0.1060 - val_accuracy: 0.9828\n",
      "Epoch 25/1000\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 4e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0566 - accuracy: 0.9979 - val_loss: 0.1057 - val_accuracy: 0.9818\n"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 1000\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "                validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "                patience=3, learning_rate_decay=True, alpha=alpha,\n",
    "                save_best=True, filepath='network1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b1d339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model(network, filename):\n",
    "    \"\"\"Save model.\"\"\"\n",
    "    K.models.save_model(network, filename)\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"Load model.\"\"\"\n",
    "    model = K.models.load_model(filename)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4edfa1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1775 - accuracy: 0.9636 - val_loss: 0.1673 - val_accuracy: 0.9700\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'format'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [43], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     27\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_oh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_valid_oh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m save_model(network, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnetwork2.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m network\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[0;32mIn [41], line 71\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(network, data, labels, batch_size, epochs, validation_data, early_stopping, patience, learning_rate_decay, alpha, decay_rate, save_best, filepath, verbose, shuffle)\u001b[0m\n\u001b[1;32m     67\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39mfilepath,\n\u001b[1;32m     68\u001b[0m                                              save_best_only\u001b[38;5;241m=\u001b[39msave_best)\n\u001b[1;32m     69\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(checkpoint)\n\u001b[0;32m---> 71\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/.env/holbertonschool-machine_learning-zF0IEfUY/lib/python3.8/site-packages/keras/engine/training.py:1230\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1227\u001b[0m   val_logs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m   1228\u001b[0m   epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n\u001b[0;32m-> 1230\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_epoch_end(epoch, epoch_logs)\n\u001b[1;32m   1231\u001b[0m training_logs \u001b[39m=\u001b[39m epoch_logs\n\u001b[1;32m   1232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/.env/holbertonschool-machine_learning-zF0IEfUY/lib/python3.8/site-packages/keras/callbacks.py:413\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    411\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_logs(logs)\n\u001b[1;32m    412\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 413\u001b[0m   callback\u001b[39m.\u001b[39;49mon_epoch_end(epoch, logs)\n",
      "File \u001b[0;32m~/.env/holbertonschool-machine_learning-zF0IEfUY/lib/python3.8/site-packages/keras/callbacks.py:1368\u001b[0m, in \u001b[0;36mModelCheckpoint.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1367\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_freq \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m-> 1368\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_model(epoch\u001b[39m=\u001b[39;49mepoch, batch\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/.env/holbertonschool-machine_learning-zF0IEfUY/lib/python3.8/site-packages/keras/callbacks.py:1403\u001b[0m, in \u001b[0;36mModelCheckpoint._save_model\u001b[0;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[1;32m   1401\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1402\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs_since_last_save \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1403\u001b[0m filepath \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_file_path(epoch, batch, logs)\n\u001b[1;32m   1405\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1406\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_best_only:\n",
      "File \u001b[0;32m~/.env/holbertonschool-machine_learning-zF0IEfUY/lib/python3.8/site-packages/keras/callbacks.py:1458\u001b[0m, in \u001b[0;36mModelCheckpoint._get_file_path\u001b[0;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[1;32m   1453\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1454\u001b[0m   \u001b[39m# `filepath` may contain placeholders such as `{epoch:02d}`,`{batch:02d}`\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m   \u001b[39m# and `{mape:.2f}`. A mismatch between logged metrics and the path's\u001b[39;00m\n\u001b[1;32m   1456\u001b[0m   \u001b[39m# placeholders can cause formatting to fail.\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m   \u001b[39mif\u001b[39;00m batch \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m logs:\n\u001b[0;32m-> 1458\u001b[0m     file_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath\u001b[39m.\u001b[39;49mformat(epoch\u001b[39m=\u001b[39mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mlogs)\n\u001b[1;32m   1459\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1460\u001b[0m     file_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilepath\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1461\u001b[0m         epoch\u001b[39m=\u001b[39mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, batch\u001b[39m=\u001b[39mbatch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mlogs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'format'"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    network = load_model('network1.h5')\n",
    "    batch_size = 32\n",
    "    epochs = 1000\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "                validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "                patience=2, learning_rate_decay=True, alpha=0.001)\n",
    "    save_model(network, 'network2.h5')\n",
    "    network.summary()\n",
    "    print(network.get_weights())\n",
    "    del network\n",
    "\n",
    "    network2 = load_model('network2.h5')\n",
    "    network2.summary()\n",
    "    print(network2.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dbeab254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(network, data, labels, verbose=True):\n",
    "    \"\"\"Test model.\"\"\"\n",
    "    result = network.evaluate(data, labels, verbose=verbose)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e2ac540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 0.1013 - accuracy: 0.9831\n",
      "[0.10127848386764526, 0.9830999970436096]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_test = datasets['X_test']\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    Y_test = datasets['Y_test']\n",
    "    Y_test_oh = one_hot(Y_test)\n",
    "\n",
    "    network = load_model('network1.h5')\n",
    "    print(test_model(network, X_test, Y_test_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b352a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, data, verbose=False):\n",
    "    \"\"\"Make prediction.\"\"\"\n",
    "    prediction = network.predict(data, verbose=verbose)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1fd9d449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.3719714e-07 5.4780651e-07 1.4773923e-05 ... 9.9989641e-01\n",
      "  6.9536975e-07 1.1765170e-05]\n",
      " [3.9417483e-08 3.3900873e-05 9.9995100e-01 ... 4.2623856e-09\n",
      "  1.0504880e-06 7.9508747e-13]\n",
      " [3.9598663e-06 9.9860507e-01 9.3497118e-05 ... 5.6158868e-04\n",
      "  5.5966200e-04 5.2904920e-07]\n",
      " ...\n",
      " [3.8534544e-12 2.0532471e-09 1.1840533e-11 ... 3.3902825e-07\n",
      "  9.8977182e-09 1.5435379e-06]\n",
      " [8.0749821e-08 3.7733518e-08 1.0712753e-09 ... 1.4866238e-07\n",
      "  2.7013547e-04 1.9977902e-09]\n",
      " [7.9525972e-07 1.5763383e-09 8.7963537e-07 ... 1.4232055e-11\n",
      "  1.8371952e-07 3.4404530e-09]]\n",
      "[7 2 1 ... 4 5 6]\n",
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_test = datasets['X_test']\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "Y_test = datasets['Y_test']\n",
    "\n",
    "network = load_model('network1.h5')\n",
    "Y_pred = predict(network, X_test)\n",
    "print(Y_pred)\n",
    "print(np.argmax(Y_pred, axis=1))\n",
    "print(Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('holbertonschool-machine_learning-zF0IEfUY')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1bb7f88ff86f8a65293f6958f33bcf6ed9c9a574270708a89302acb2663e3cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
